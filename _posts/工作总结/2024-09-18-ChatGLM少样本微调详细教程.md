---
layout: post
title: "ChatGLM少样本微调详细教程"
date: 2024-09-18
author: cora Liu
categories: [工作总结]
---


## 一、环境准备

第一步先安装与 **ChatGLM** 和微调相关的库，包括 `transformers`、`accelerate`、`datasets`、`peft` 和 `torchkeras`，以便进行`模型预训练`和`模型微调`。

```shell
# chatglm
!pip install transformers

# finetune
!pip install -U accelerate
!pip install datasets
!pip install -U peft 
!pip install -U torchkeras 
```

### 导入常用模块
接着导入常用的 `pytorch`、 `numpy`、`pandas` 等常用库。

```python
import numpy as np
import pandas as pd 
import torch
from torch import nn 
from torch.utils.data import Dataset,DataLoader 
```

### 参数配置

相关的参数配置如下，进行训练时调参就是调这里啦！

```python
from argparse import Namespace
cfg = Namespace()

#dataset
cfg.prompt_column = 'prompt'
cfg.response_column = 'response'
cfg.history_column = None
cfg.source_prefix = '[prompt前缀]' #添加到每个prompt开头的前缀引导语
cfg.source_prefix = ''

cfg.max_source_length = 64 
cfg.max_target_length = 128

# 模型相关
cfg.model_name_or_path = 'd:\\cache\\Models\\chatglm2-6b'  #远程'THUDM/chatglm-6b' 
cfg.data_path = 'e:/code/gpt/chatGLM-6B-QLoRA/data/swf.pmt-resp.jsonl'
cfg.quantization_bit = None #量化，存储精度，仅仅预测时可以选 4 or 8 


# 训练相关（预训练&微调）
cfg.epochs = 50 
cfg.lr = 5e-3 # 学习率
cfg.batch_size = 1 
cfg.gradient_accumulation_steps = 16 #梯度累积
```

## 二、模型加载

- 加载配置和分词器：

    - 使用 `AutoConfig` 从预训练模型路径加载配置。
    - 使用 `AutoTokenizer` 从同一路径加载分词器，准备处理文本数据。

- 加载模型：

    - 使用 `AutoModel` 从预训练路径加载模型，并设置为半精度（half()），以节省内存和提高计算效率。

- 量化处理：

    - 如果指定了量化位数（`cfg.quantization_bit`），则对模型进行量化，以进一步减少模型大小和加快推理速度。

- 移动到 `GPU：`
    - 将模型转移到 `GPU` 上，以利用 `GPU` 的计算能力。

- 创建 `ChatGLM` 实例：

    - 通过 `torchkeras` 库创建一个 `ChatGLM` 类的实例，传入已加载的模型和分词器，以便进行对话生成等任务。

相关代码如下 ⬇️

```python
import transformers
from transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq


config = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(
    cfg.model_name_or_path, trust_remote_code=True)

model = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,
                                  trust_remote_code=True).half() 

#量化瘦身
if cfg.quantization_bit is not None:
    print(f"Quantized to {cfg.quantization_bit} bit")
    model = model.quantize(cfg.quantization_bit)
    
#移动到GPU上
model = model.cuda();
print(tokenizer.eos_token_id, tokenizer.pad_token_id)

# 创建了一个 ChatGLM 类的实例，传入了预先加载的模型（model）和分词器（tokenizer）
from torchkeras.chat import ChatGLM 
chatglm = ChatGLM(model,tokenizer)
```
## 三、数据准备

### 1、加载数据
- 定义 `jsonl_load` 函数：该函数接受一个文件路径作为参数，使用 `jsonlines` 库逐行读取文件内容，将每个 `JSON` 对象存储到列表 `ret` 中，最终返回该列表。

- 加载数据：调用 `jsonl_load` 函数，使用配置中的数据路径 (`cfg.data_path`) 加载数据，并将结果存储在 data 变量中。

- 创建 `DataFrame`：使用 `Pandas` 将加载的数据转换为 `DataFrame` 格式，便于后续的数据分析和处理。

```python
import pandas as pd
import jsonlines
def jsonl_load(file):
    ret = []
    with jsonlines.open(file) as lines:
        for obj in lines:
            ret.append(obj)
    return ret
# data =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]
data = jsonl_load(cfg.data_path)
dfdata = pd.DataFrame(data)
display(dfdata)
```

数据示例如下:
```json
{"prompt": "你好，请问今天的天气如何？", "response": "今天的天气非常好，晴朗且微风。"}
{"prompt": "你能告诉我什么是黑洞吗？", "response": "黑洞是由大质量恒星塌缩形成的天体，它的引力非常强，连光也无法逃逸。"}
{"prompt": "请帮我生成一个购物清单。", "response": "你的购物清单可以包括：牛奶、面包、鸡蛋和水果。"}

```

### 2、构建测试集和验证集

将数据划分为测试集以及验证集，相关代码如下 ⬇️

```python
import datasets 
ds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)
```

### 3、数据预处理

对数据进行预处理，可以提高模型训练的效率和效果，确保输入数据的质量一致性，从而减少噪声对模型性能的影响。

它还可以帮助将原始数据转换为模型能够理解的格式，为训练和推理提供基础。

```python

def preprocess(examples, cfg=cfg, tokenizer=tokenizer):
    max_seq_length = cfg.max_source_length + cfg.max_target_length
    model_inputs = {
        "input_ids": [],
        "labels": [],
    }
    for i in range(len(examples[cfg.prompt_column])):
        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:
            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]

            history = examples[cfg.history_column][i] if cfg.history_column is not None else None
            prompt = tokenizer.build_prompt(query, history)

            prompt = cfg.source_prefix + prompt
            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,
                                     max_length=cfg.max_source_length)
            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,
                                     max_length=cfg.max_target_length)

            context_length = len(a_ids)
            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]
            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]

            pad_len = max_seq_length - len(input_ids)
            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
            labels = labels + [tokenizer.pad_token_id] * pad_len
            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]
            model_inputs["input_ids"].append(input_ids)
            model_inputs["labels"].append(labels)
    return model_inputs

ds_train = ds_train_raw.map(
    preprocess,
    batched=True,
    num_proc=4,
    remove_columns=ds_train_raw.column_names
)

ds_val = ds_val_raw.map(
    preprocess,
    batched=True,
    num_proc=4,
    remove_columns=ds_val_raw.column_names
)
```

上述代码定义了一个 `preprocess` 函数，用于处理输入示例并生成模型所需的 `input_ids` 和 `labels`，以便进行训练。它通过构建提示、编码文本、填充到固定长度，并将填充的标签替换为 -100，以适应模型输入格式。

### 4、构建数据加载器 **DataLoader**
`DataLoader` 在 `PyTorch` 中用于将数据集打包为小批量（`batch`），并提供高效的迭代、数据加载和多线程并行处理，方便在模型训练和评估时逐批处理数据。

```python
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=None,
    label_pad_token_id=-100,
    pad_to_multiple_of=None,
    padding=False
)

dl_train = DataLoader(ds_train,batch_size = cfg.batch_size,
                      num_workers = 2, shuffle = True, collate_fn = data_collator 
                     )
dl_val = DataLoader(ds_val,batch_size = cfg.batch_size,
                      num_workers = 2, shuffle = False, collate_fn = data_collator 
                     )
```


## 四、开始微调 **FineTuning**

下面我们使用 `AdaLoRA` 方法来微调 `ChatGLM2`。

**LoRA（Low-Rank Adaptation）** 是一种通过低秩矩阵来进行模型微调的方法，减少了模型参数更新的计算量和存储需求。

**AdaLoRA** 是在 LoRA 的基础上引入自适应学习率机制，能够在训练过程中动态调整每个层的学习率，以提高微调效果和收敛速度。

#### 1、AdaLoRA配置
```python
from peft import get_peft_model, AdaLoraConfig, TaskType

#训练时节约GPU占用
model.config.use_cache=False
model.supports_gradient_checkpointing = True  #
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

peft_config = AdaLoraConfig(
    task_type=TaskType.CAUSAL_LM, inference_mode=False,
    r=8,
    lora_alpha=32, lora_dropout=0.1,
    target_modules=["query", "value"]
)

peft_model = get_peft_model(model, peft_config)

peft_model.is_parallelizable = True
peft_model.model_parallel = True
peft_model.print_trainable_parameters()
```

### 2、模型加载与保存

为了更加高效地保存和加载参数，这里覆盖了`KerasModel`中的`load_ckpt`和`save_ckpt`方法，

仅仅保存和加载可训练`lora`权重，这样可以避免加载和保存全部模型权重造成的存储问题。

```python

from torchkeras import KerasModel 
from accelerate import Accelerator 

class StepRunner:
    def __init__(self, net, loss_fn, accelerator=None, stage = "train", metrics_dict = None, 
                 optimizer = None, lr_scheduler = None, tokenizer=tokenizer
                 ):
        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage
        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler
        self.accelerator = accelerator if accelerator is not None else Accelerator() 
        if self.stage=='train':
            self.net.train() 
        else:
            self.net.eval()
        self.tokenizer = tokenizer
        self.i = 0
    
    def __call__(self, batch):
        self.i += 1
        sents = []
        #loss
        with self.accelerator.autocast():
            ret = self.net(input_ids=batch["input_ids"],labels=batch["labels"])
            if self.stage != 'train' and self.i > 20:
                inp_ids = batch['input_ids'].cpu().numpy()
                pred_ids = torch.argmax(ret.logits, dim=-1).cpu().numpy()
                for i in range(pred_ids.shape[0]):
                    src = self.tokenizer.decode(inp_ids[i])
                    stop = np.where(pred_ids==self.tokenizer.eos_token_id)[0]
                    
                    sent = self.tokenizer.decode(pred_ids[i][:stop])
                    print(src, sent)
                    # sents.append(sent)
            loss = ret.loss

        #backward()
        if self.optimizer is not None and self.stage=="train":
            self.accelerator.backward(loss)
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)
            self.optimizer.step()
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            
        all_loss = self.accelerator.gather(loss).sum()
        
        #losses (or plain metrics that can be averaged)
        step_losses = {self.stage+"_loss":all_loss.item()}
        
        #metrics (stateful metrics)
        step_metrics = {}
        
        if self.stage=="train":
            if self.optimizer is not None:
                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']
            else:
                step_metrics['lr'] = 0.0
        # step_metrics['sents'] = str(sents)
        return step_losses,step_metrics
    
KerasModel.StepRunner = StepRunner 


#仅仅保存lora可训练参数
def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):
    unwrap_net = accelerator.unwrap_model(self.net)
    unwrap_net.save_pretrained(ckpt_path)
    
def load_ckpt(self, ckpt_path='checkpoint'):
    import os
    self.net.load_state_dict(
        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)
    self.from_scratch = False
    
KerasModel.save_ckpt = save_ckpt 
KerasModel.load_ckpt = load_ckpt 

optimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) 
keras_model = KerasModel(peft_model,loss_fn = None,
        optimizer=optimizer) 
ckpt_path = 'coramini'
```

### 3、开始训练

传入相关的参数开始训练。
下述代码使用 `Keras` 的 `fit` 方法训练模型，指定训练数据和验证数据、训练周期数以及早停机制（patience 参数）。它还设置了检查点路径、混合精度训练（fp16）、以及梯度累积步数，以提高训练效率和性能。

```python
keras_model.fit(train_data = dl_train,
                val_data = dl_val,
                epochs=cfg.epochs,
                patience=5,
                monitor='val_loss',
                mode='min',
                ckpt_path = ckpt_path,
                mixed_precision='fp16',
                gradient_accumulation_steps = cfg.gradient_accumulation_steps
               )
```

### 4、提取模型并保存

```python
from peft import PeftModel 
from transformers import AutoModel
from argparse import Namespace
from torchkeras.chat import ChatGLM 
cfg = Namespace()
#model
cfg.model_name_or_path = 'd:\\cache\\Models\\chatglm2-6b'  #远程'THUDM/chatglm-6b' 
cfg.ckpt = 'gji'
# ckpt_path = 
model_old = AutoModel.from_pretrained(cfg.model_name_or_path,
                                  load_in_4bit=False, 
                                  trust_remote_code=True)
peft_loaded = PeftModel.from_pretrained(model_old, cfg.ckpt).cuda()
model_new = peft_loaded.merge_and_unload() #合并lora权重

tokenizer = AutoTokenizer.from_pretrained(
    cfg.model_name_or_path, trust_remote_code=True)
chatglm = ChatGLM(model_new, tokenizer, max_chat_rounds=0, temperature=0.01) #支持多轮对话，可以从之前对话上下文提取知识。
```

### 5、保存模型
训练完把模型权重保存起来，记得同时把配置文件等移动到对应的文件夹中。
```python
save_path = "chatglm2-6b-adalora"
model_new.save_pretrained(save_path, max_shard_size='2GB')
```

通过微调 `ChatGLM`，我们能够在特定任务上实现更优的性能，充分发挥其强大的生成能力和理解能力。随着技术的不断发展和应用场景的扩展，`ChatGLM` 的微调方法为多种自然语言处理任务提供了灵活的解决方案。