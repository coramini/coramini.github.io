---
layout: post
title: "语言模型的本质—概率"
date: 2024-06-10
author: cora Liu
categories: [机器学习基础]
---


语言模型的本质在于通过概率来描述和生成自然语言。具体来说，它通过学习文本数据中的词语和短语的出现概率，来预测和生成自然语言。让我们深入了解一下：

### 1. 概率基础

**核心思想：**
- 语言模型的核心是概率，它通过计算词语序列的出现概率来理解和生成语言。例如，在一句话中，给定前面的词语，模型会计算下一个词语出现的概率。

**举个例子：**
- 考虑一句话“我今天早上吃了一个”。语言模型会计算“苹果”在这个上下文中出现的概率。假设“苹果”出现的概率是0.4，“香蕉”是0.3，“面包”是0.2，等等。

<img src="/assets/imgs/ai/llm/lm-01.png" />

### 2. 词语序列的概率

**n-gram模型：**
- 一种简单的语言模型是n-gram模型，它基于词语序列中的n个词来计算下一个词的概率。例如，二元模型（bigram）只考虑前一个词，三元模型（trigram）考虑前两个词。

**示例：**
- 在bigram模型中，给定词语“吃了”，模型会计算“早上”在其后的概率，即P(吃了|早上)。
<img src="/assets/imgs/ai/llm/bigram.png" />
- 在trigram模型中，给定词语序列“今天早上”，模型会计算“吃了”在其后的概率，即P(吃了|今天早上)。
<img src="/assets/imgs/ai/llm/trigram.png" />
### 3. 条件概率

**条件概率：**
- 语言模型使用条件概率来描述词语在特定上下文中的出现概率。条件概率表示在已知某些条件下某事件发生的概率。

**公式：**
- 对于给定的词语序列 $w_1, w_2, ..., w_n$，语言模型计算整个序列的概率：
  $
  P(w_1, w_2, ..., w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdot ... \cdot P(w_n|w_1, w_2, ..., w_{n-1})
  $

### 4. 训练语言模型

**学习概率：**
- 通过大量的文本数据，语言模型学习每个词语及其上下文的概率分布。现代语言模型如BERT和GPT通过深度学习和大规模语料库来学习复杂的概率关系。

**示例：**
- 给定一大段文本，模型会统计词语及其上下文的频率，从而估计这些词语序列的条件概率。例如，如果“苹果”在“我今天早上吃了一个”这个上下文中出现的次数很多，模型会给出“苹果”较高的概率。

### 5. 损失的计算

下面进一步解释一下语言模型的输入和输出，以及损失计算中的输入和输出。

**详细说明：**

假设我们的输入是“我今天早上吃了一个”，真实标签是“苹果”。

1. **模型预测：**
   - 给定输入“我今天早上吃了一个”，模型预测下一个词的概率分布：
     $
     P(苹果) = 0.4, \quad P(香蕉) = 0.3,  \quad P(面包) = 0.2
     $

2. **真实标签：**
   - 真实标签是“苹果”，即：
     $
     P(苹果) = 1, \quad P(香蕉) = 0,  \quad P(面包) = 0
     $

3. **计算交叉熵损失：**
   - 交叉熵损失公式为：
     $
     Loss = - \sum 真实标签 \times \log(模型预测概率)
     $
   - 对于“苹果”：损失为 $-1 \times \log(0.4)$
   - 对于“香蕉”：损失为 $-0 \times \log(0.3)$ （因为真实标签为0，所以这项不计入损失）
   - 对于“面包”：损失为 $-0 \times \log(0.2)$ （同理）

总损失就是这些损失的求和。由于只有“苹果”对损失有贡献，所以损失为：
$
Loss = - \log(0.4)
$

### 6. 生成语言

**使用概率生成语言：**
- 当生成语言时，模型根据计算出的概率来选择词语。例如，在给定开头“我今天早上吃了一个”后，模型会根据概率选择最有可能的下一个词。假设“苹果”出现的概率最高，那么模型会选择“苹果”作为下一个词。

<img src="/assets/imgs/ai/llm/lm-01.png" />


### 7. 拓展：由一维到二维
在神经网络或机器学习模型中，**二维向量的概率分布形状**通常指的是模型的输出为多个样本的概率分布。特别是当我们对一批样本（mini-batch）进行预测时，每个样本的概率分布会成为一个向量，而多个样本的预测则组成一个二维矩阵。

#### 1. **二维向量的定义**
在多分类问题中，如果我们处理的是多个样本，模型的输出可以用一个**二维矩阵**来表示：

- **行**：表示样本的索引，每一行对应一个样本的预测结果。
- **列**：表示类别的索引，每一列对应一个类别的预测概率。

因此，整个输出是一个形状为 \( (n, K) \) 的二维向量，其中 \(n\) 是样本数（mini-batch 大小），\(K\) 是类别数。

#### 2. **二维向量的概率分布**
对于一个 mini-batch 的样本集，每个样本的预测概率分布是一个**一维概率向量**，即每个样本对所有类别的预测概率。对于多个样本，这些一维概率向量组合在一起就形成了一个**二维矩阵**，每一行是一个样本的概率分布。

##### 举例：
假设我们有一个分类问题，类别数 \(K = 3\)，且模型处理一个 mini-batch 包含 \(n = 4\) 个样本。模型最后的输出是一个 \(4 \times 3\) 的矩阵：

\[
\mathbf{P} =
\begin{bmatrix}
0.7 & 0.2 & 0.1 \\
0.1 & 0.5 & 0.4 \\
0.3 & 0.4 & 0.3 \\
0.6 & 0.1 & 0.3 \\
\end{bmatrix}
\]

在这个矩阵中，每一行代表一个样本的概率分布：
- 第一行 \( [0.7, 0.2, 0.1] \) 表示第一个样本的预测结果，模型认为它属于类别1的概率最大（70%）。
- 第二行 \( [0.1, 0.5, 0.4] \) 表示第二个样本属于类别2的概率最大（50%）。
- 其他行同理。

<img src="/assets/imgs/ai/llm/2-dim-prob.png" />

每一行的数值表示模型对不同类别的置信度，所有行加起来是一个完整的预测矩阵。

#### 3. **二维向量概率分布的形状**
这种矩阵的**几何形状**和**概率特性**有以下几方面的特征：

- **行的总和为1**：每一行的所有元素（即每个类别的概率）加起来应该等于1，因为每个样本的预测结果是一个有效的概率分布。例如，第一行的概率 \(0.7 + 0.2 + 0.1 = 1\)。

- **每个元素是概率**：每个元素代表了样本属于某个类别的概率，取值范围在 [0, 1] 之间。

#### 4. **几何解释与概率分布的形状**
二维向量的概率分布可以通过几何视角解释：

- **每一行的概率向量**：对于每个样本，其概率向量可以看作是在 \(K\)-维简单x上的一个点。对于分类问题中的三个类别，它可以被看作是嵌在**二维平面的概率三角形**内的点，这种几何形状表明概率总和为1。

- **矩阵的行与列**：
  - **行**表示不同样本，每个样本都有一个属于各个类别的概率分布。
  - **列**表示不同类别，每个列中有多个概率，代表多个样本对这个类别的预测。

#### 5. **图形化表示**
当可视化二维向量概率分布时，我们可以通过图形直观地理解模型的输出：

- **柱状图**：对于每一个样本，可以用柱状图来展示它对不同类别的概率。例如，每一行可以用一个柱状图来表示，其中每个柱子对应一个类别的概率。
  
- **热力图（Heatmap）**：对于整个二维概率分布矩阵，可以用热力图来表示。热力图中的每个格子颜色深浅表示概率的大小，这样可以直观地看到哪个样本对哪些类别的预测更高。

##### 例子：
对于上面的 \(4 \times 3\) 概率矩阵，我们可以画一个热力图：

- **x轴**：类别索引（如类别1、类别2、类别3）。
- **y轴**：样本索引（如样本1、样本2、样本3、样本4）。
- **颜色**：每个格子的颜色表示该样本对某类别的预测概率，颜色越深表示概率越大。

#### 6. **应用场景**
- **分类任务**：在分类任务中，二维向量的概率分布经常出现在多分类场景，表示多个样本的分类概率。例如在图像分类任务中，模型可能同时处理多个图片，输出每个图片对不同类别的概率。

- **序列数据**：在处理序列数据（如自然语言处理、时间序列预测）时，每个时间步/序列元素都有一个概率分布，这些概率分布可以形成一个二维矩阵（或更高维度）。

#### 小结

- **二维向量的概率分布形状**通常出现在模型处理多个样本时，输出是一个二维矩阵，每一行是一个样本对各个类别的预测概率，每一列代表某个类别在所有样本中的概率。
- **行向量的总和为1**：每个样本的概率分布都是归一化的，表示属于各个类别的概率。
- **热力图或柱状图**可以帮助我们直观理解这些概率分布。

### 总结

语言模型的本质在于通过概率来描述和生成自然语言。它通过学习大量文本数据中的词语和短语的出现概率，来预测下一个词语并生成连贯的句子。现代语言模型如BERT和GPT通过深度神经网络和大规模语料库，更加精确地捕捉语言中的复杂概率关系，从而实现高效的语言理解和生成。
