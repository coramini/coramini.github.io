---
layout: post
title: "了解梯度消失与梯度爆炸"
date: 2023-09-10
author: cora Liu
categories: [机器学习基础]
---
## 梯度消失
梯度消失是指在反向传播过程中，较深层的神经元接收到的梯度非常小，以至于它们的权重几乎没有被更新，导致网络学习缓慢，甚至停止学习。

### 原因
造成梯度消失的原因主要有两个：激活函数和网络架构。

#### 激活函数
当使用`sigmoid`等激活函数时，它们在输入**远离零点时**梯度变得非常小，从而导致梯度消失。

常见激活函数(**activation function**)、损失函数(**loss function**)与梯度(**gradient**)相关关系如下图所示 ⬇️

<img src="/assets/imgs/ai/activation&loss&gradient.png" width="400" />

#### 网络结构

深度网络的反向传播会将梯度逐层传递，导致梯度被多次乘以较小的权重矩阵，这也会导致梯度消失。

### 解决方案

使用其他的激活函数，例如`ReLU`、`Leaky ReLU`和`ELU`等，这些函数在输入大于0时梯度不会消失，从而可以避免梯度消失的问题。

## 梯度爆炸
梯度爆炸则是指反向传播过程中，梯度变得非常大，从而导致数值不稳定。这可能会导致网络参数的更新变得不可预测，训练过程变得不稳定。
### 原因
梯度爆炸的原因通常是由于网络权重过大或网络架构中存在矩阵相乘产生数值过大的情况，例如**RNN中时间步过多**或**权重初始化不合理**等。

### 解决方案

- **批归一化（batch normalization）** 可以使得每层的输入数据分布在均值为0，方差为1的范围内，从而使得梯度更容易被传递。
- **权重初始化技巧** 例如使用Xavier初始化可以使得权重更加合理地初始化，从而避免梯度爆炸。
- **梯度裁剪（gradient clipping）** 可以限制梯度的大小，从而避免梯度爆炸。
- 使用**更浅的网络架构、更加稀疏的连接以及其他正则化技巧**也可以减少梯度消失和梯度爆炸的问题。

