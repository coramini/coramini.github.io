---
layout: post
title: "一文了解 RNN、LSTM以及GRU"
date: 2023-06-25
author: cora Liu
categories: [机器学习基础]
pin: true
---


## 一、RNN
`RNN（循环神经网络）` 是一种用于处理`序列数据`的神经网络。如`字符串序列`、`单词序列`、`时间序列`，都是常见的序列数据。


`RNN`是由若干个基础单元组成的，在每个基础单元（时间步）中，`RNN`接收输入 $X_t$ 和先前隐藏状态 $h_t$，计算输出和更新后的隐藏状态 $h_{t-1}$。即下图中的 $h_0$、$h_1$、...、$h_4$, 并传递给下一个时间步。如下图所示，输入为 $X$，输出为 `O`。

<img src="/assets/imgs/ai/RNN/RNN-1.png" width="300" />

那么基础单元里面到底是怎么计算的呢？我们把 `RNN` 模型的基础单元展开来看。

<img src="/assets/imgs/ai/RNN/RNN-2.png" width="600" />

1、隐藏状态 $h_t$

> $h_t = tanh(W_{xh}X_t + W_{hh}h_{t-1})$

其中，$tanh$是激活函数，$h_t$ 是关于上一个记忆隐藏状态 $h_{t-1}$ 和 本次输入 $X_t$ 的函数。

2、输出 $O_t$
> $O_t = W_{oh}h_t + b_o$


## 二、LSTM 
在长序列中，传统的 `RNN` 模型只有有限的记忆能力，无法有效地保持长期的记忆。

另外,传统的 `RNN` 还存在**梯度消失**和**梯度爆炸**问题，当 `RNN` 模型在反向传播过程中通过时间展开计算梯度时，梯度信息会不断地乘以权重矩阵，导致梯度指数级地衰减或增长。

为了解决这个问题，科学家们提出了很多种`RNN`的变种，其中有一种就是`LSTM`。`LSTM（Long Short-Term Memory）` 即长短期记忆网络。

<img src="/assets/imgs/ai/LSTM/LSTM.png" width="500" />

`LSTM` 与 `RNN` 相比，引入了一个长期记忆的状态 $C_t$，使得前面的序列的信息得以记忆不丢失。当然计算过程也有所不同。

`LSTM` 的基础单元由**遗忘门（forget gate）**、**输入门（input gate）**、**输出门（output gate）** 所组成。接下来分别看看不同门单元的计算过程。

### 1、遗忘门（forget gate）



<img src="/assets/imgs/ai/LSTM/LSTM-ft.png" width="500" />

上图中，$f_t$ 称为**遗忘因子**。遗忘门的激活值向量 $f_t$ 决定了前一时刻的记忆单元 $c_{t-1}$ 中的哪些信息应该被遗忘。

- 当 $f_t$ 的元素接近 `0` 时，对应位置的信息会被遗忘。
- 当 $f_t$ 的元素接近 `1` 时，对应位置的信息会被保留。

### 2、输入门
输入门（Input Gate）是长短期记忆网络（`LSTM`）中的一个关键组件，它用于控制选择性地更新记忆单元中的信息。下面是输入门的计算过程。

<img src="/assets/imgs/ai/LSTM/LSTM-input.png" width="500" />


激活值向量 $i_t$ 可以看作是一个控制开关，控制着记忆单元对于当前时刻输入的敏感程度。每个元素的取值范围在 `[0, 1]` 之间，其中接近 `0` 表示不重要，接近 `1` 表示重要。

- 当激活值向量 $i_t$ 的某个元素接近于 `1` 时，表示相应位置的输入信息被认为是重要的，并且会被选择性地传递到记忆单元中。

- 当激活值向量 $i_t$ 的某个元素接近于 `0` 时，表示相应位置的输入信息被认为是不重要的，将被过滤掉。

候选值：$\widetilde{C_t}$。这个候选值表示可以用来更新记忆单元的新信息。通过将当前时刻的输入与相应的权重相乘并加上偏置项来计算得到的。

候选值的计算是为了从输入中提取可能对当前时刻有用的特征，并准备将其纳入到记忆单元中。


### 3、$C_t$ 长期记忆

$C_t$是长期记忆线，作为一个新的变量保存需要长期记忆的信息。

<img src="/assets/imgs/ai/LSTM/LSTM-ct.png" width="500" />


$C_t$ 可以看作是一个包含记忆和遗忘的容器。它通过**输入门**和**遗忘门**的控制来决定如何更新和保留信息。

### 4、输出门
输出门（Output Gate）的原理是，根据输入和记忆状态计算输出。

<img src="/assets/imgs/ai/LSTM/LSTM-ot.png" width="500" />


其中，$O_t$ 是输出门的激活值。输出门激活值向量 $O_t$ 的直观理解是控制隐藏状态向量 $h_t$ 中的哪些信息应该被输出或传递给下一层。

**最终的真正输出通常与输出门激活值向量 $O_t$ 相关。**

- 当输出门激活值向量 $O_t$ 的某个元素接近于 `1` 时，表示隐藏状态向量 $h_t$ 对应位置的信息被认为是重要的，并会被选择性地输出或传递给下一层。
- 当输出门激活值向量 $O_t$ 的某个元素接近于 `0` 时，表示对应位置的信息被认为是不重要的，将被过滤掉。


$h_t$： $h_t$ 可以看作是经过输出门调节后的新隐藏状态。它由$C_t$ 与 $O_t$计算得出，如下图所示：

<img src="/assets/imgs/ai/LSTM/LSTM-ht.png" width="500" />

介绍完 `LSTM`，我们来看看 `GRU`，`GRU`即`（Gated Recurrent Unit）`。

## 三、GRU
`GRU`可以看成是 `LSTM`的简化版，`GRU`相对于 `LSTM` 具有更简化的结构，所以计算效率有所提升。

然而，`LSTM` 在处理长期依赖关系方面具有更好的记忆性能，因此，我们需要在不同场景下选择不同的计算方法。

`GRU` 由 **重置门** 和 **更新门** 两个门单元组成。

### 1、重置门

重置门的作用是决定是否忽略上一时刻的隐藏状态，重置隐藏状态的一部分。
<img src="/assets/imgs/ai/GRU/GRU-1.png" width="500" />

重置门可以看作是一个开关，用于决定在当前时刻是否需要重置隐藏状态的一部分，以适应当前的输入。每个元素的取值范围在 `[0, 1]` 之间。
- 当重置门激活值向量 $r_t$ 的某个元素接近于 `1` 时，表示对应位置的信息在当前时刻的输入中是重要的，需要被保留，并用于计算候选隐藏状态。

- 当重置门激活值向量 $r_t$ 的某个元素接近于 `0` 时，表示对应位置的信息应该被忽略，不对最终的隐藏状态产生影响。



### 2、更新门
更新门的作用是决定当前时刻的隐藏状态应该多少程度上受到输入的影响。
<img src="/assets/imgs/ai/GRU/GRU-1.png" width="500" />

更新门可以看作是一个调节器，用于控制当前时刻输入的重要性，以更新隐藏状态。

- 当更新门激活值向量 $z_t$ 的某个元素接近于 `1` 时，表示对应位置的输入信息在当前时刻是重要的，并应该完全用于更新隐藏状态。

- 当更新门激活值向量 $z_t$ 的某个元素接近于 `0` 时，表示对应位置的输入信息在当前时刻是不重要的，应该被忽略。

### 3、候选隐藏状态$\widetilde{h_t}$

候选隐藏状态表示当前时刻的可能隐藏状态，它会被用于更新最终的隐藏状态 $h_t$。

<img src="/assets/imgs/ai/GRU/GRU-2.png" width="500" />

### 4、更新隐藏状态 $h_t$

计算新的隐藏状态向量 $h_t = (1 - z_t) ⊙ h_{t-1} + z_t ⊙ \widetilde{h_t}$。


<img src="/assets/imgs/ai/GRU/GRU-3.png" width="500" />

其中 `⊙`表示逐元素相乘。

这个公式表示当前时刻的隐藏状态由两部分组成：$(1 - z_t) ⊙ h_{t-1}$ 表示保持上一时刻的隐藏状态的一部分，$z_t ⊙ \widetilde{h_t}$ 表示采纳候选隐藏状态的一部分。


## 小结

传统的`RNN`在处理长序列时面临**梯度消失**或**梯度爆炸**的问题，导致难以有效地捕捉长期依赖关系。`LSTM`通过引入一个称为"记忆单元"（memory cell）的结构来解决这个问题。

`GRU（Gated Recurrent Unit）`和 `LSTM（Long Short-Term Memory）`都是用于处理序列数据的递归神经网络`（RNN）`的变体。它们在设计上有些相似，但也存在一些区别。

选择 `GRU` 还是 `LSTM` 取决于具体问题的特点、数据集的属性以及实际的性能需求。
