---
layout: post
title: "详解梯度下降法"
date: 2023-05-31
author: cora Liu
categories: [机器学习基础]
---

> 在机器学习中，最常听到的模型优化的方法就是“**梯度下降法**”，本文详细介绍了梯度下降法的由来，以及控制梯度变化速率的相关方法——**学习率（learning rate）**。


在机器学习中，通常有以下几个主要的步骤 ⬇️

<img src="/assets/imgs/ai/机器学习基本步骤.png"  />

其中包括了：

- 数据收集
- 数据分析
- 数据预处理
- 模型搭建
- 模型训练
- 模型优化
- 模型评估
- 模型存储

而 `梯度下降法（Gradient Descent）` 是在 **【模型优化】** 的步骤中得到广泛应用。它在**模型参数更新**的时候发挥了重要的作用。

在 `梯度下降法（Gradient Descent）` 中，梯度是指 **损失函数** 对于参数的**偏导数**。那么，我们先来看看什么是损失函数。

## 损失函数

在机器学习和深度学习中，**损失函数（Loss Function）** 是一个用来度量模型`预测结果`与`真实值`之间差异的函数。它衡量了模型在训练过程中的错误程度，越小表示模型的预测结果与真实值越接近。

损失函数的选择取决于任务的性质和模型的目标。不同的任务和模型可能需要使用不同的损失函数。那么在线性回归和逻辑回归中，损失函数是怎么定义的。

### 线性回归的损失函数

在线性回归中，常用的损失函数是**均方误差（Mean Squared Error，MSE）**。均方误差衡量了模型的预测值与真实值之间的平方差的平均值。

**均方误差**的定义如下：

> MSE = (1/m) * Σ(ŷi - yi)²

- 其中，Σ 表示对所有样本求和，m 表示样本的数量。


假设我们有一个包含`m`个样本的训练集。

<img src="/assets/imgs/ai/gradient_descent/y.jpg"  width="250" style="display:block;"/>


我们用一个线性模型来进行预测，模型的预测值为 `ŷ`，真实值为 `y`。对于第 i 个样本，其预测值和真实值之间的差异可以表示为 `ŷi - yi`。

<img src="/assets/imgs/ai/gradient_descent/y_predict.jpg"  width="350" style="display:block;"/>


在这里，这个线性回归模型中的损失函数就是`MSE`。在模型优化的过程中，我们需要使得损失函数`MSE`最小。即，以下式子计算结果最小：

> MSE = (1/m) * Σ(ŷi - yi)² = (1/m) * Σ(axi + b - yi)²

这里可以用使用`微积分`中的求导求最小值来进行求解。


### 逻辑回归的损失函数

在逻辑回归中，常用的损失函数是 **二分类交叉熵损失函数（Binary Cross-Entropy Loss）**。该损失函数用于衡量逻辑回归模型的预测概率与真实标签之间的差异。

**二分类交叉熵损失函数** 的定义如下：

> CE = - (1/m) *Σ(y* log(ŷ) + (1-y) * log(1-ŷ))

假设我们有一个包含m个样本的训练集，对于逻辑回归问题，我们使用 **sigmoid函数** 将线性模型的输出转换为概率值，即将输出值映射到[0, 1]之间的范围。

<img src="/assets/imgs/ai/逻辑回归/sigmoid函数图像.png" style="display:block;"/>


假设模型的预测概率为 ŷ，真实标签为 y（0或1）。


那么，如果 `y=0`，那么 `ŷ` 越接近0，损失 `y * log(ŷ) = 0`， `(1-y) * log(1-ŷ)` 越接近0。若此时 `ŷ` 趋近1，损失 `y * log(ŷ) = 0，(1-y)* log(1-ŷ)` 趋向 -∞。

<img src="/assets/imgs/ai/gradient_descent/cross_y_0.jpg" width="500" style="display:block;"/>


同理可得，如果 `y=1`，那么 `ŷ` 越接近1，损失 `(1-y) * log(1-ŷ) = 0`， `y * log(ŷ)` 越接近0。若此时 `ŷ` 趋近0，损失 `(1-y) * log(1-ŷ) = 0`，`y * log(ŷ)` 趋向 -∞。

<img src="/assets/imgs/ai/gradient_descent/cross_y_1.jpg" width="500" style="display:block;"/>

该函数的计算结果，可以用一个表格简单表示一下 ⬇️
| y | ŷ | CE |
|--|--|--|
|0|0|0|
|0|1|∞|
|1|0|∞|
|1|1|0|


逻辑回归模型中的损失函数就是`CE`。在模型优化的过程中，我们需要使得损失函数最小，则CE取值越小越好。同样也可以用`微积分`中的导数求小值的方法来求解。



## 微积分求最值

在上面损失函数中，如果我们需要获取在某个定义域内函数的最值，可以通过微积分来求解。

根据微积分导数原理，可以通过以下步骤求解函数的最值：

- 1、找到函数的关键点：计算函数的`导数`，确定`导数为零`或`不存在的点`。

- 2、判断关键点的`极值类型`：利用第一导数测试或第二导数测试来判断关键点的`极值类型`。

- 3、比较函数在`关键点处的值`以及`定义域的边界值`：将关键点的函数值与函数定义域内的边界值进行比较，找到最大值或最小值。

<img src="/assets/imgs/ai/高等数学/微积分/微积分求最值.jpg"  width="600" style="display:block;"/>

从上图可以看到，定义域的边界值，与导数为0的极值非常重要，而**梯度**正是由导数的相关概念引入的一个新的概念。


## 何为“梯度”
**对于一元函数来说，梯度表示函数在某一点的变化率或斜率。**

<img src="/assets/imgs/ai/高等数学/微积分/切线.png"  style="display:block;"/>

**对于多元函数来说，梯度是一个向量，包含函数在每个自变量方向上的偏导数。**

对于一个二元函数 f(x, y)，梯度可以表示为 `(∂f/∂x, ∂f/∂y)`。

类似地，对于一个三元函数 `f(x, y, z)`，梯度可以表示为 `(∂f/∂x, ∂f/∂y, ∂f/∂z)`。

**可以直观地理解为，梯度表示的是在各个自变量上损失函数变化最大的方向。**


## 何为“下降”
那么，什么是下降呢？在梯度下降法中，下降指的是通过迭代更新自变量（参数）的取值，使得损失函数的值逐渐减小，直至达到最小值或收敛。

## Learning Rate

我们知道，**梯度就是损失函数变化最大的方向**。通常我们沿着梯度的反方向更新参数：

```
θ_new = θ_old - gradient
```

这样一来，可以得到新的参数`θ_new`。那么如果我们想要控制参数更新的速率的话，可以在`gradient`的基础上乘上一个数值，也就是**学习率**。

```
θ_new = θ_old - learning_rate * gradient
```

其中，`θ_new` 表示更新后的参数值，`θ_old` 表示上一次迭代的参数值，`learning_rate` 表示学习率，`gradient` 表示当前点的梯度。

### Learning Rate 取值
> 学习率是梯度下降法中的一个重要参数，用于控制每次参数更新的步长或跨度。它决定了在每次迭代中，自变量（参数）沿着梯度方向更新的程度。


学习率的选择很重要，它会影响梯度下降算法的性能和收敛速度。


<img src="/assets/imgs/ai/gradient_descent/LR.png"  style="display:block;"/>

如果学习率过小，每次更新的步长会很小，可能导致收敛速度很慢，需要更多的迭代次数才能达到最优解。

另一方面，如果学习率过大，每次更新的步长会很大，可能导致在最优解附近发生震荡或无法收敛。

通常，学习率需要经过调试和实验来确定。一种常用的做法是使用**固定的学习率**，但也有一些改进的算法（如**自适应学习率算法**，如`AdaGrad`、`Adam`等）可以自动地根据梯度的情况来调整学习率，从而提高算法的性能和稳定性。
