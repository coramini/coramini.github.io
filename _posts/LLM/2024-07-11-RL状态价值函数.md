---
layout: post
title: "RL状态价值函数"
date: 2024-07-11
author: cora Liu
categories: [LLM, RL]
---

## 马尔可夫决策过程
**马尔可夫决策过程（MDP, Markov Decision Process）** 是强化学习的核心框架之一，用来描述一个具有决策能力的智能体与其所处环境的交互过程。MDP为解决决策问题提供了数学模型，特别适用于环境具有随机性且决策影响长期结果的场景。

### 1. **MDP的定义**
MDP由以下四个部分组成：
- **S**：状态空间（State space），表示环境可能的所有状态集合。
- **A**：动作空间（Action space），表示智能体在各个状态下可以执行的动作集合。
- **P(s'|s, a)**：状态转移概率（Transition probability），表示在状态`s`执行动作`a`后转移到新状态`s'`的概率。
- **R(s, a)**：奖励函数（Reward function），表示在状态`s`下执行动作`a`获得的即时奖励。
- **γ（gamma）**：折扣因子（Discount factor），用来衡量当前奖励和未来奖励的相对重要性，范围是0到1之间。值越接近1，表示对未来奖励的重视程度越高。

## MDP与状态价值函数
### 1. **价值函数**
为了评估策略的好坏，MDP定义了两种常见的价值函数：
- **状态价值函数 V(s)**：表示智能体从状态`s`开始，按照策略π行动能获得的长期期望回报。它表示为：
  $
  V^{\pi}(s) = \mathbb{E}^{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right]
  $
- **状态-动作价值函数 Q(s, a)**：表示智能体在状态`s`下执行动作`a`，然后按照策略π行动能获得的长期期望回报。它表示为：
  $
  Q^{\pi}(s, a) = \mathbb{E}^{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, a_0 = a \right]
  $

### 2. **MDP中的目标**
智能体的目标是找到一个最优策略`π*`，使得从任何初始状态出发，能够最大化长期累计奖励。这可以通过求解**最优价值函数**和**最优策略**来实现：
- 最优状态价值函数： $V^*(s)$
- 最优状态-动作价值函数： $Q^*(s, a)$
  
这些函数可以通过**贝尔曼方程**和**动态规划**、**Q-learning**、**策略梯度方法**等算法进行求解。


## RL状态价值函数计算方法
- 贝尔曼方程
- 蒙特卡洛方法
- 动态规划方法
- 时间差分方法

## 贝尔曼方程
**贝尔曼方程**是描述最优决策问题中的递归关系的核心方程，用于强化学习和动态规划中，来计算状态的价值或最优策略。它是基于最优子结构的原理，将问题分解为更小的子问题进行求解。

贝尔曼方程有两种形式：**状态价值函数的贝尔曼方程**和**状态-动作价值函数的贝尔曼方程**。

### 1. **状态价值函数的贝尔曼方程** $V(s)$
贝尔曼方程定义了在状态$ s $的价值为当前即时奖励$ R(s) $与未来所有可能状态的折扣奖励的和：
$
V(s) = \mathbb{E}[R(s) + \gamma \cdot V(s')]
$
其中：
- $ V(s) $ 是状态$ s $的价值，即从状态$ s $开始遵循某策略的期望累积回报。
- $ R(s) $ 是状态$ s $下的即时奖励。
- $ \gamma $ 是折扣因子，衡量未来奖励的重要性（取值范围 $0 \leq \gamma < 1$）。
- $ s' $ 是执行动作后转移到的下一状态。
- 期望$ \mathbb{E} $表示考虑了未来所有可能的状态转移。

### 2. **状态-动作价值函数的贝尔曼方程** $Q(s, a)$
状态-动作价值函数的贝尔曼方程定义了在状态$ s $选择动作$ a $的价值为即时奖励和未来所有可能状态的最大折扣价值：
$
Q(s, a) = \mathbb{E}[R(s, a) + \gamma \cdot \max_{a'} Q(s', a')]
$
其中：
- $ Q(s, a) $ 是状态$ s $下选择动作$ a $的价值。
- $ R(s, a) $ 是执行动作$ a $后获得的即时奖励。
- $ \max_{a'} Q(s', a') $ 是从下一状态$ s' $出发能获得的最大未来回报。

### 贝尔曼方程的意义
贝尔曼方程的关键思想是：当前状态的价值等于即时奖励加上未来状态的折扣价值。这种递归结构使得在动态规划和强化学习中可以通过迭代计算来求解最优策略或最优值函数。

在强化学习中，基于贝尔曼方程的算法如**Q-learning**和**Sarsa**等，通过不断更新 $V(s)$ 或 $Q(s, a)$ 来逼近最优值。


## 蒙特卡洛方法
在强化学习中的蒙特卡洛方法，**状态价值函数 $V(s)$** 的计算是基于经验的，主要通过多次采样不同的episode，计算从状态 $s$ 开始的累积回报的平均值来估计该状态的价值。

### 蒙特卡洛方法计算状态价值函数的步骤

1. **生成完整episode**：
   蒙特卡洛方法要求智能体从起始状态开始，按照策略执行动作，直到达到终止状态。每个episode的状态-动作-奖励序列被记录下来。

   一个episode可以表示为：$(s_0, a_0, r_1), (s_1, a_1, r_2), ..., (s_T, a_T, r_{T+1})$
   其中：
   - $s_t$ 是时间步$t$时的状态。
   - $a_t$ 是时间步$t$时的动作。
   - $r_{t+1}$ 是从时间步$t$到$t+1$时获得的奖励。
   - $T$ 是episode的终止时间步。

2. **计算每个状态的回报**：
   对于每一个状态 $s_t$，从该状态开始到episode结束的总累积回报 $G_t$ 是：
   $
   G_t = r_{t+1} + \gamma \cdot r_{t+2} + \gamma^2 \cdot r_{t+3} + \cdots + \gamma^{T-t} \cdot r_{T+1}
   $
   其中 $ \gamma $ 是折扣因子，用于衡量未来奖励的重要性。

3. **更新状态价值函数**：
   每次访问某个状态 $s$ 时，记录该状态的回报 $G_t$，并计算从该状态 $s$ 开始的所有回报的平均值。对于状态价值函数 $V(s)$，蒙特卡洛方法通过以下公式更新：
   $
   V(s) \leftarrow V(s) + \alpha \cdot (G_t - V(s))
   $
   其中：
   - $ \alpha $ 是学习率，用于控制每次更新的步长。
   - $ G_t $ 是从状态 $s$ 开始的累积回报。

   如果没有使用学习率，每个状态 $s$ 的值就是它从所有episode中观察到的回报的平均值：
   $
   V(s) = \frac{1}{N} \sum_{i=1}^{N} G_t
   $
   其中 $N$ 是状态 $s$ 被访问的次数。

## 动态规划方法
**动态规划**（Dynamic Programming，简称DP）方法在强化学习中用于计算**状态价值函数** $ V(s) $ 或**状态-动作价值函数** $ Q(s, a) $，依赖于**贝尔曼方程**和环境的动态模型（状态转移概率和奖励函数）。动态规划通常适用于已知环境模型的情况下，通过迭代地更新价值函数来求解最优策略。


### 1. **动态规划计算价值函数的特点**

- **依赖模型**：动态规划要求已知环境的动态模型（即状态转移概率 $P(s'|s, a)$ 和奖励函数 $R(s, a, s')$）。
- **确定性**：动态规划是确定性的，因为它依赖于精确的模型和贝尔曼方程，每次更新都是基于所有可能的转移情况进行的。
- **全局更新**：在每次迭代中，所有状态的价值函数都会被更新，且更新时考虑了所有可能的转移和奖励。


总而言之，可以简单理解为基于贝尔曼方程的全局更新方法，适用于确定的环境env的场景。


## 时间差分TD方法

**Q-learning** 是基于 **TD** 方法的一种具体的算法，属于时间差分控制算法的一部分。

**TD**方法结合了蒙特卡洛和动态规划的优点，它可以逐步更新价值函数，不需要等待整个**episode**结束。

