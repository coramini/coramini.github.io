---
layout: post
title: "显存优化之ZeRO"
date: 2024-06-06
author: Cola Liu
categories: [LLM]
---

## Zero-Redundancy Optimizer

**Zero-Redundancy Optimizer (ZeRO)** 是一种用于深度学习模型训练的技术，旨在显著降低内存消耗和提高计算效率。`ZeRO` 技术由微软的 `DeepSpeed` 团队开发，可以在训练大规模深度学习模型时显著减少内存占用。`ZeRO` 分为三个阶段，每个阶段在减少内存占用和提高计算效率方面都有不同的优化策略。

以下是 `ZeRO` 的三个阶段及其应用 ⬇️


### 1、ZeRO-1 (Optimizer State Partitioning)
在 `ZeRO-1` 阶段，优化器状态被分区到不同的设备中。通常，优化器状态包括动量和自适应矩阵（如 Adam 优化器的状态）。

这些状态的大小通常是模型参数大小的两倍，因此通过分区可以大幅减少单个设备上的内存占用。

具体处理过程可以见下图 ⬇️

<img src="/assets/imgs/ai/llm/ZeRO/zero-stage1.png"/>

#### Forward
- 复制模型参数，数据分片 
- 每个`GPU`处理一部分
#### Backward (先更新参数得到梯度)
- 反向传播得到梯度 **(全部计算完)**
- 通过 `Reduce Scatter` 更新 `Gradient*`
- 每个卡更新其中一部分
- 更新的时候通过 `All Gather` 更新

### 2、ZeRO-2 (Gradient Partitioning)
在 `ZeRO-2` 阶段，梯度被分区到不同的设备中。在传统的分布式数据并行`（DDP）`中，每个设备计算完整的梯度并进行同步。

而在 `ZeRO-2` 中，梯度被分割，每个设备只保存和处理一部分梯度，从而进一步减少内存占用。

> `ZeRO-1`与`ZeRO-2`的区别在于梯度的计算与保存，`ZeRO-2`更节约内存。


具体处理过程如下图 ⬇️

<img src="/assets/imgs/ai/llm/ZeRO/zero-stage2.png"/>

#### Forward
- 复制模型参数，数据分片 
- 每个`GPU`处理一部分
#### Backward(分层更新梯度，然后去除Gradient)
- 反向传播得到梯度 **(分层计算)**
- 通过 `Reduce Scatter` 更新 `Gradient*`
- 每个卡更新其中一部分
- 更新的时候通过 `All Gather` 更新

### 3、ZeRO-3 (Parameter Partitioning)
在 `ZeRO-3` 阶段，模型参数本身被分区到不同的设备中。这意味着每个设备只保存和处理一部分模型参数，而不是整个模型。这可以显著降低内存使用，使得更大规模的模型能够在相对较小的设备上进行训练。
具体过程如下图 ⬇️

<img src="/assets/imgs/ai/llm/ZeRO/zero-stage3.png"/>

#### Forward
- 数据分片，每个卡是模型参数的一部分
- 通过 `All Gather` 获得模型全部参数(多一次计算)
- 计算完不需要全部参数了，可以只保持一部分

#### Backward(分层更新梯度，然后去除Gradient)
- 反向传播得到梯度 **(分层计算)**
- 通过 `Reduce Scatter` 更新   `Gradient*`
- 每个卡更新其中一部分
- 更新的时候通过 `All Gather` 更新

### 应用与优势

1. **大规模模型训练**：
   `ZeRO` 技术的主要应用是在训练大规模深度学习模型（如 `GPT-3`、`BERT` 等）时，减少内存占用和计算成本。通过将优化器状态、梯度和模型参数分区，`ZeRO` 可以使得单个 `GPU` 或 `TPU` 能够处理更大规模的模型，从而提高训练效率。

2. **分布式训练**：
   `ZeRO` 的各个阶段都可以与分布式训练相结合，进一步提高训练效率和模型可扩展性。在分布式训练中，多个设备协同工作，`ZeRO` 通过优化内存使用，减少了通信开销，提升了训练速度。

3. **节省资源**：
   通过减少内存占用，`ZeRO` 技术可以降低硬件资源的需求，节省计算成本。这对于资源有限的研究团队和企业来说非常重要。

### 示例代码

以下是使用 `DeepSpeed` 实现 `ZeRO-1` 的示例代码：

```python
import deepspeed
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 10)

    def forward(self, x):
        return self.fc(x)

# 创建模型和优化器
model = SimpleModel()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# 配置 ZeRO-1
ds_config = {
    "train_batch_size": 8,
    "zero_optimization": {
        "stage": 1,
    },
    "fp16": {
        "enabled": True,
    },
}

# 使用 DeepSpeed 初始化模型和优化器
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    optimizer=optimizer,
    config=ds_config
)

# 训练循环
for epoch in range(10):
    inputs = torch.randn(8, 10).cuda()
    outputs = model_engine(inputs)
    loss = outputs.mean()
    model_engine.backward(loss)
    model_engine.step()
```

### 总结

`Zero-Redundancy Optimizer (ZeRO)` 技术通过在优化器状态、梯度和模型参数上的分区，显著降低了深度学习模型训练的内存占用，并提高了训练效率。它在大规模模型训练和分布式训练中具有重要应用。