---
layout: post
title: "模型剪枝"
date: 2024-06-27
author: Cola Liu
categories: [LLM]
---


<img src="/assets/imgs/ai/llm/prune/prune-0.png"/>


`模型剪枝（Model Pruning）`是一种减少深度神经网络模型参数和计算量的方法。通过剪除冗余或不重要的神经元或连接，可以在保持模型性能的同时，提高模型的计算效率，减少存储需求，并加快推理速度。


## 模型剪枝的主要方法

模型剪枝主要分为**非结构化剪枝**以及**结构化剪枝**。下面分别介绍一下这两种剪枝的具体操作。

### 1、非结构化剪枝
非结构化剪枝，就是按单个权重剪枝，即直接将小于某个阈值的权重设置为零。非结构化剪枝可以获得较高的压缩率，但生成的稀疏矩阵需要特殊的硬件或软件支持来实现高效计算。

<img src="/assets/imgs/ai/llm/prune/prune-weight.png"/>


从上图的权重剪枝实验结果可以看出，**30-40%** 的参数都可以舍弃，舍弃后在下游任务上效果也不会变差。

#### 代码实现

代码实现上来看，有很多现成的工具包可以直接使用，下面介绍一下`PyTorch`的用法。`PyTorch` 提供了`torch.nn.utils.prune`模块来进行非结构化剪枝。


```python
import torch
import torch.nn.utils.prune as prune
import torch.nn.functional as F

# 定义模型
model = torch.nn.Sequential(...)

# 选择要剪枝的参数
parameters_to_prune = (
    (model.layer1, 'weight'),
    (model.layer2, 'weight'),
)

# 应用剪枝
for module, param in parameters_to_prune:
    prune.l1_unstructured(module, name=param, amount=0.5)

# 剪枝后再训练
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
for epoch in range(num_epochs):
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()

```
### 2、结构化剪枝

<img src="/assets/imgs/ai/llm/prune/prune-2.png"/>

结构化剪枝，就是按`神经元`、`卷积核`或整个`通道`剪枝。这种方法剪去整个结构单元，生成的模型仍然是密集的，因而更容易与现有硬件和软件兼容。

下图是一些简单的结构化剪枝的思路。



<img src="/assets/imgs/ai/llm/prune/prune-1.png"/>

除了常见的`神经元`、`卷积核`或整个`通道`剪枝。还有 `Attention head`、`模块`、`神经网络层`剪枝等。相关操作如下图所示 ⬇️

<img src="/assets/imgs/ai/llm/prune/prune-3.png"/>


从下面相关的实验结果可以看到，`Attention head`个数不是越多越好，要根据具体的结果数据进行微调。


<img src="/assets/imgs/ai/llm/prune/prune-attention.png"/>



总而言之，结构化剪枝技术在实际应用中能够有效地提升深度学习模型的效率和性能，尤其适合于需要在资源有限的设备上部署和运行的场景。