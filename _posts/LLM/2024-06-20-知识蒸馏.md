---
layout: post
title: "知识蒸馏"
date: 2024-06-20
author: cora Liu
categories: [LLM]
---

## 写在前面
知识蒸馏（`Knowledge Distillation`）是一种模型压缩技术，用于将一个复杂的大模型（称为`Teacher`模型）的知识传递给一个较小的模型（称为`Student`模型）。

其主要目的是在保持性能的前提下，减少模型的复杂性和大小，从而加快推理速度并降低计算资源的需求。

## 核心思想
在知识蒸馏过程中，`Teacher`模型和`Student`模型的训练目标不仅仅是传统的标签数据，还包括`Teacher`模型的预测输出。具体步骤如下：

- 训练`Teacher`模型：首先在大规模数据集上训练一个性能优异的`Teacher`模型。这通常是一个深度且复杂的模型，如BERT。

- 生成软标签`soft targets`：使用训练好的`Teacher`模型对训练数据进行预测，生成软标签（soft targets），这些软标签是`Teacher`模型输出的概率分布。

- 训练`Student`模型：在训练`Student`模型时，除了使用原始的硬标签（`hard targets`），还使用`Teacher`模型生成的软标签。通过最小化`Student`模型的输出与软标签之间的差异，`Student`模型能够学习到更多细微的知识。

具体过程如下图所示 ⬇️
<img src="/assets/imgs/ai/llm/KD/kd-0.png"/>

## 损失函数

知识蒸馏通常使用加权损失函数来综合hard targets(hard labels)和软标签soft targets(soft labels)的信息。总损失可以表示为：

$ Loss = \alpha \cdot Loss_{hard} + (1 - \alpha) \cdot Loss_{soft} $

<img src="/assets/imgs/ai/llm/KD/kd-1.png"/>

其中：
- $Loss_{hard}$ 是学生模型预测与硬标签之间的交叉熵损失。**在上图中，$Loss_{hard}$ 计算为 $L_{CE}(y,p)$。**

- $Loss_{soft}$ 是学生模型预测与软标签之间的交叉熵损失。**在上图中，$Loss_{soft}$ 为 $L_{CE}(q,p)$。**

- $\alpha$ 是权重因子，用于平衡两种损失。


## softmax-T
在知识蒸馏中，`softmax-T（softmax with temperature）`主要体现在Teacher模型生成`soft targets`的过程中。通过引入温度参数 $ T $，可以调节教师模型输出的概率分布，使得学生模型能够更好地学习到教师模型的细微知识。

`softmax-T`的计算公式如下：

$ softmax_T(z_i) = \frac{\exp(z_i / T)}{\sum_{j} \exp(z_j / T)} $

<img src="/assets/imgs/ai/llm/KD/softmax-t.png"/>

通过调节温度参数 $ T $：
- **较低的温度（ $ T < 1 $ ）**：使得输出概率分布更加陡峭，较大的logit值对应的概率更大，较小的logit值对应的概率更小。
- **较高的温度（ $ T > 1 $ ）**：使得输出概率分布更加平滑，logit值之间的差异减小，更加均匀。


> 通过softmax-T，知识蒸馏能够更有效地传递复杂模型的知识，提升学生模型的性能。


## 应用：BERT 迁移到 BiLSTM

BERT模型非常庞大，因此计算速度较慢，知识蒸馏可以将模型进行压缩，使其迁移到更小的模型上。

下面简单介绍一下通过知识蒸馏，将BERT模型的知识迁移到BiLSTM模型上。[论文出处戳这里](https://arxiv.org/abs/1903.12136)

### BiLSTM模型-1
<img src="/assets/imgs/ai/llm/KD/bilstm-1.png"/>

### BiLSTM模型-2

<img src="/assets/imgs/ai/llm/KD/bilstm-2.png"/>

上述模型比较简单，主要在于损失函数有些微区别。
- 简单使用`softmax`函数，没有上`softmax-T`
- $Loss_{soft}$ 使用的是`MSE（mean square error）`而不是交叉熵CE。

<img src="/assets/imgs/ai/llm/KD/kd-2.png"/>

总的来说，知识蒸馏是一种让大模型教小模型的方法，能让小模型变得更聪明但保持简单。大模型通过生成软标签帮助小模型更好地学习，从而提升性能。