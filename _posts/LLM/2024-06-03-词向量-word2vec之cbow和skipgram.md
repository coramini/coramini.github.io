---
layout: post
title: "è¯å‘é‡-word2vecä¹‹cbowå’Œskipgram"
date: 2024-06-03
author: Cola Liu
categories: [LLM]
---
## è¯å‘é‡

è¯å‘é‡ **ï¼ˆWord Embeddingï¼‰** æ˜¯ä¸€ç§å°†è¯è¯­è¡¨ç¤ºä¸ºå®æ•°å‘é‡çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å°†è¯è¯­æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´ç©ºé—´ä¸­ï¼Œä½¿å¾—åœ¨è¯¥ç©ºé—´ä¸­å…·æœ‰ç›¸ä¼¼æ„ä¹‰çš„è¯è¯­çš„å‘é‡å½¼æ­¤æ¥è¿‘ã€‚

å¦‚æœä¸ç”¨å‘é‡çš„æ–¹æ³•æ¥è¡¨ç¤ºï¼Œé‚£ä¹ˆè¯ä¸è¯ä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼Œå°±æ²¡æ³•å¾ˆå¥½åœ°è¡¨ç¤ºäº†ã€‚

### æœ€åŸºç¡€çš„è¡¨ç¤ºæ–¹æ³•â€”one-hot

åœ¨`one-hot`ç¼–ç ä¸­ï¼Œæ¯ä¸ªç±»åˆ«éƒ½è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªä¸ç±»åˆ«æ•°é‡ç›¸åŒé•¿åº¦çš„äºŒè¿›åˆ¶å‘é‡ã€‚åœ¨è¿™ä¸ªå‘é‡ä¸­ï¼Œåªæœ‰ä¸€ä¸ªä½ç½®çš„å€¼ä¸º1ï¼Œå…¶ä½™ä½ç½®çš„å€¼å‡ä¸º0ã€‚

ä¸‹é¢ä¸¾ä¸ªç®€å•çš„ ğŸŒ°

> å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªç±»åˆ«çš„åˆ†ç±»ç‰¹å¾ï¼š["çŒ«", "ç‹—", "å…”å­"]ã€‚ä½¿ç”¨one-hotç¼–ç ï¼Œå¯ä»¥å°†è¿™ä¸‰ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºå¦‚ä¸‹çš„äºŒè¿›åˆ¶å‘é‡ â¬‡ï¸

- çŒ« -> [1, 0, 0]
- ç‹— -> [0, 1, 0]
- å…”å­ -> [0, 0, 1]

ä»ä¸Šé¢ä¾‹å­å¯ä»¥çœ‹åˆ°ï¼Œå‘é‡ä¸å‘é‡ä¹‹é—´æ˜¯äº’ç›¸å‚ç›´çš„ï¼Œå®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§æ— æ³•ä½“ç°ã€‚

åŒæ—¶ï¼Œå½“ç±»åˆ«æ•°é‡è¾ƒå¤šæ—¶ï¼Œ`one-hot`ç¼–ç ä¼šäº§ç”Ÿéå¸¸é«˜ç»´åº¦çš„ç¨€ç–å‘é‡ï¼Œå¯èƒ½ä¼šå¢åŠ è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚

é‚£ä¹ˆå‘é‡ä¸å‘é‡ä¹‹é—´çš„ç›¸å…³æ€§åœ¨å“ªä½“ç°å‘¢ï¼Œæˆ‘ä»¬å¯ä»¥ä»ç‚¹ä¹˜è·ç¦»å’Œæ¬§æ°è·ç¦»çš„è§’åº¦æ¥çœ‹çœ‹ï½

#### 1ã€ç‚¹ä¹˜è·ç¦»
ç‚¹ä¹˜è·ç¦»ï¼Œå…¶å®å°±æ˜¯è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼ˆä¹Ÿå«ç‚¹ç§¯ï¼‰ã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆæˆ‘ä»¬åœ¨æ¯”è¾ƒä¸¤ä¸ªå‘é‡åœ¨åŒä¸€ä¸ªæ–¹å‘ä¸Šçš„ç›¸ä¼¼ç¨‹åº¦ã€‚å¦‚æœä¸¤ä¸ªå‘é‡çš„æ–¹å‘ç›¸ä¼¼ï¼Œç‚¹ä¹˜çš„ç»“æœä¼šæ¯”è¾ƒå¤§ï¼›å¦‚æœæ–¹å‘ç›¸åï¼Œç»“æœä¼šæ¯”è¾ƒå°ï¼Œç”šè‡³æ˜¯è´Ÿæ•°ã€‚

#### 2ã€æ¬§æ°è·ç¦»

æ¬§æ°è·ç¦»å°±æ˜¯æˆ‘ä»¬å¹³å¸¸è¯´çš„â€œç›´çº¿è·ç¦»â€ã€‚å®ƒæµ‹é‡çš„æ˜¯ä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿è·ç¦»ã€‚å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨åœ°å›¾ä¸Šçœ‹ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„æœ€çŸ­è·ç¦»ï¼Œé‚£å°±æ˜¯æ¬§æ°è·ç¦»ã€‚

#### 3ã€ä¸¾ä¸ªå°ä¾‹å­ ğŸŒ°

å‡è®¾ä½ å’Œæœ‹å‹åœ¨äºŒç»´å¹³é¢ä¸Šï¼Œä½ åœ¨(1, 2)ç‚¹ï¼Œä»–åœ¨(4, 6)ç‚¹ã€‚

- **æ¬§æ°è·ç¦»**ï¼šå¯ä»¥ç”¨å‹¾è‚¡å®šç†è®¡ç®—ï¼Œ$\sqrt{(4-1)^2 + (6-2)^2}$ï¼Œä¹Ÿå°±æ˜¯$\sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$ã€‚æ‰€ä»¥ä½ ä»¬ä¹‹é—´çš„ç›´çº¿è·ç¦»æ˜¯5ã€‚
- **ç‚¹ä¹˜è·ç¦»**ï¼šå‡è®¾ä½ ä»¬çš„å‘é‡åˆ†åˆ«æ˜¯$[1, 2]$å’Œ$[4, 6]$ã€‚ç‚¹ä¹˜è·ç¦»å°±æ˜¯ $1*4 + 2*6 = 4 + 12 = 16$ã€‚è¿™ä¸ªå€¼è¡¨ç¤ºä½ ä»¬åœ¨â€œé€›å•†åœºåå¥½â€ä¸Šçš„ç›¸ä¼¼ç¨‹åº¦ã€‚


### é™æ€è¯å‘é‡ vs åŠ¨æ€è¯å‘é‡

#### 1ã€é™æ€è¯å‘é‡
é™æ€è¯å‘é‡ï¼Œå°±åƒæ¯ä¸ªè¯éƒ½æœ‰ä¸€å¼ å›ºå®šçš„åç‰‡ã€‚æ— è®ºåœ¨ä»€ä¹ˆè¯­å¢ƒä¸‹ï¼Œè¿™å¼ åç‰‡ä¸Šçš„ä¿¡æ¯ï¼ˆå‘é‡ï¼‰éƒ½æ˜¯ä¸å˜çš„ã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„è¡¨ç¤ºæ–¹æ³•ï¼Œè¿™ä¸ªè¡¨ç¤ºæ–¹æ³•ä¸ä¼šéšç€ä¸Šä¸‹æ–‡çš„å˜åŒ–è€Œå˜åŒ–ã€‚

> æ¯”å¦‚ï¼Œâ€œè‹¹æœâ€è¿™ä¸ªè¯åœ¨æ‰€æœ‰çš„å¥å­é‡Œéƒ½æ˜¯ç”¨åŒä¸€ä¸ªå‘é‡æ¥è¡¨ç¤ºï¼Œæ— è®ºå®ƒæ˜¯åœ¨â€œæˆ‘åƒäº†ä¸€ä¸ªè‹¹æœâ€è¿˜æ˜¯â€œè‹¹æœå…¬å¸å‘å¸ƒäº†æ–°äº§å“â€è¿™æ ·çš„å¥å­é‡Œï¼Œå‘é‡éƒ½æ˜¯ä¸€æ ·çš„ã€‚

å¸¸è§çš„é™æ€è¯å‘é‡æ¨¡å‹æœ‰**Word2Vec**ã€**GloVe**ç­‰ã€‚

#### 2ã€åŠ¨æ€è¯å‘é‡
åŠ¨æ€è¯å‘é‡åˆ™æ›´èªæ˜ï¼Œå®ƒä¼šæ ¹æ®è¯åœ¨å¥å­ä¸­çš„å…·ä½“è¯­å¢ƒæ¥è°ƒæ•´è‡ªå·±ã€‚

>æ¯”å¦‚ï¼Œâ€œè‹¹æœâ€è¿™ä¸ªè¯åœ¨â€œæˆ‘åƒäº†ä¸€ä¸ªè‹¹æœâ€é‡Œè¡¨ç¤ºæ°´æœï¼›è€Œåœ¨â€œè‹¹æœå…¬å¸å‘å¸ƒäº†æ–°äº§å“â€é‡Œï¼Œâ€œè‹¹æœâ€æŒ‡çš„æ˜¯å…¬å¸ã€‚

å¸¸è§çš„åŠ¨æ€è¯å‘é‡æ¨¡å‹æœ‰**BERT**ã€**GPT**ç­‰ã€‚


ä¸‹é¢ä¸»è¦ä»‹ç» `Word2Vec` ä¸­ä¸¤ç§æ¨¡å‹ï¼š`CBOW` å’Œ `skip-gram`ã€‚


## CBOW
`BOWï¼š bag of word`ï¼Œç»Ÿè®¡è¯é¢‘ã€‚

`CBOWï¼ˆContinuous Bag of Wordsï¼‰`æ˜¯`Word2Vec`ä¸­çš„ä¸€ç§æ¨¡å‹ï¼Œå®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡æ¥é¢„æµ‹å½“å‰è¯æ±‡ã€‚å¦‚æœé‡‡ç”¨çš„æ˜¯  `trigramç­–ç•¥`ï¼Œé‚£ä¹ˆæ ¹æ®å‰åä¸¤ä¸ªè¯æ¥é¢„æµ‹å½“å‰è¯æ±‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º â¬‡ï¸

<img src="/assets/imgs/ai/llm/cbow.png" />


## skip-gram

å®ç°`Word2Vec`æ¨¡å‹é™¤äº†`CBOW`ï¼Œ è¿˜æœ‰ `Skip-gram`ã€‚å®ƒè·Ÿ`CBOW`æ˜¯åè¿‡æ¥çš„ï¼Œå³é€šè¿‡å½“å‰è¯æ±‡å»é¢„æµ‹ä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º â¬‡ï¸

<img src="/assets/imgs/ai/llm/skip-gram.png" />

æ¥ä¸‹æ¥æ¼”ç¤ºä¸€ä¸‹å¦‚ä½•ä½¿ç”¨PyTorchå®ç°Word2Vecçš„Skip-gramæ¨¡å‹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡æ•°æ®å’Œå®šä¹‰ä¸€äº›è¶…å‚æ•°ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# å®šä¹‰è¯­æ–™åº“
corpus = [
    'I like deep learning',
    'I like NLP',
    'I enjoy coding',
    'I enjoy writing'
]

# æ„å»ºè¯æ±‡è¡¨
word_list = ' '.join(corpus).split()
vocab = list(set(word_list))
word_to_idx = {word: idx for idx, word in enumerate(vocab)}
idx_to_word = {idx: word for idx, word in enumerate(vocab)}

# è¶…å‚æ•°
embedding_dim = 100
window_size = 2
batch_size = 1
num_epochs = 100
learning_rate = 0.001
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæ•°æ®é›†ç±»æ¥å¤„ç†æ•°æ®ï¼š

```python
class Word2VecDataset(Dataset):
    def __init__(self, corpus, word_to_idx, window_size):
        self.data = []
        for sentence in corpus:
            words = sentence.split()
            for i, target_word in enumerate(words):
                context_words = [words[j] for j in range(max(i - window_size, 0), min(i + window_size + 1, len(words))) if j != i]
                for context_word in context_words:
                    self.data.append((word_to_idx[context_word], word_to_idx[target_word]))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰Word2Vecæ¨¡å‹ï¼š

```python
class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, inputs):
        embedded = self.embeddings(inputs)
        output = self.linear(embedded)
        return output
```

æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥å‡†å¤‡æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹ï¼š

```python
# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
dataset = Word2VecDataset(corpus, word_to_idx, window_size)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# åˆ›å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = SkipGramModel(len(vocab), embedding_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# è®­ç»ƒæ¨¡å‹
for epoch in range(num_epochs):
    total_loss = 0
    for context_idx, target_idx in data_loader:
        optimizer.zero_grad()
        outputs = model(context_idx)
        loss = criterion(outputs, target_idx)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}, Loss: {total_loss}')
```

åœ¨è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨æ¨¡å‹çš„è¯åµŒå…¥å±‚æ¥è·å–è¯å‘é‡ï¼š

```python
# è·å–è¯åµŒå…¥çŸ©é˜µ
embeddings = model.embeddings.weight.data.numpy()

# æŸ¥çœ‹è¯å‘é‡
for word, idx in word_to_idx.items():
    print(f'Word: {word}, Embedding: {embeddings[idx]}')
```

ä»¥ä¸Šæ˜¯ä¸€ä¸ªç®€å•çš„`Word2Vec` `Skip-gram`æ¨¡å‹çš„`PyTorch`å®ç°ç¤ºä¾‹ã€‚åœ¨å®é™…ä¸­ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹ç»“æ„ã€è¶…å‚æ•°ç­‰ï¼Œä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ã€‚


## å°ç»“
æ€»çš„æ¥è¯´ï¼Œè¯å‘é‡æ˜¯ä¸€ç§å°†è¯è¯­æ˜ å°„åˆ°é«˜ç»´è¿ç»­ç©ºé—´çš„æŠ€æœ¯ï¼Œä½¿å¾—æ¯ä¸ªè¯è¯­éƒ½å¯ä»¥ç”¨ä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚è¿™äº›å‘é‡æ•æ‰äº†è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä½¿å¾—æœºå™¨èƒ½å¤Ÿç†è§£å’Œå¤„ç†è‡ªç„¶è¯­è¨€ã€‚

å¸¸è§çš„è¯å‘é‡æŠ€æœ¯æœ‰`Word2Vec`ï¼Œ`GloVe`ï¼ˆGlobal Vectors for Word Representationï¼‰ç­‰ã€‚æœ¬æ–‡ä¸»è¦ä»‹ç»`Word2Vec`ï¼ŒåŒ…å«`CBOW`å’Œ`Skip-gram`ä¸¤ç§æ¨¡å‹ã€‚