---
layout: post
title: "RL基础概念"
date: 2024-07-04
author: cora Liu
categories: [LLM,RL]
---

## 基础概念


强化学习（`Reinforcement Learning，RL`）是一种机器学习方法，旨在通过智能体（`Agent`）与环境（`Environment`）的交互来学习如何采取行动，以最大化长期累积奖励。

强化学习的核心思想是试错学习，智能体通过与环境不断互动，获取反馈（`Reward`），并逐步调整策略（`Policy`），从而优化其行为。



<img src="/assets/imgs/ai/llm/RL/rl.png"/>

强化学习的关键概念包括以下几个方面：

**智能体（Agent）**：执行动作并从环境中获得反馈的主体。在强化学习中，智能体的目标是学会在不同的状态下采取最优的行动，以获得最大累积奖励。

**环境（Environment）**：智能体所处的外部世界，智能体通过与环境的交互获得关于当前状态的信息，并根据动作的结果获得奖励或惩罚。

**状态（State/Observation）**：环境在某一时刻的表征，描述智能体当前所处的情境。状态通常是环境通过传感器或其他方式反馈给智能体的信息。

**动作（Action）**：智能体根据当前状态采取的行动。在每个状态下，智能体可以选择不同的动作，动作会影响智能体的状态以及未来的奖励。

**奖励（Reward）**：环境对智能体所采取动作的反馈信号，用来衡量该动作的好坏。奖励可以是正值（表示好的动作）或负值（表示不好的动作）。

**策略（Policy）**：智能体根据当前状态选择动作的规则或方法。策略可以是确定性的（在相同状态下总是选择相同的动作）或随机的（在相同状态下根据概率选择不同的动作）。

**价值函数（Value Function）**：用于评估智能体在某一状态下的长期预期奖励。价值函数帮助智能体评估某个状态或动作是否有助于获得更高的长期回报。


### 应用举例

下面我们举一个 **3x5** 的迷宫游戏的例子，以此了解一下相关的概念。

我们有一个迷宫maze，迷宫中有障碍物和出口。

#### 1、Agent and Environment
在这个例子中，**Agent** 就是参加游戏的移动的智能体，**3x5** 的迷宫 maze 就是 **Environment**。

在下面的迷宫中，有障碍物也有出口，这就涉及到奖赏与惩罚 **Reward** 了，在下面会介绍。
<img src="/assets/imgs/ai/llm/RL/env_agent.png"/>

#### 2、Observation、Action and Reward
在本迷宫游戏中，**Observation(state)** 就是指每一个行动**action**中，**Agent** 所在的坐标。

由于迷宫行走有上下左右四个方向，那么对应的 **action** 有四个方向：**up、down、left、right**。

关于 **Reward**，就是整个学习过程中的奖赏与惩罚，对此我们设置**rewards**，如下所示。      


<img src="/assets/imgs/ai/llm/RL/state_action_reward.png"/>

#### 3、Q表

我们创建了一个 **3x5x4** 的 Q 表，表示 **3x5** 的状态空间，**4** 代表 4个action（上、下、左、右)。

<img src="/assets/imgs/ai/llm/RL/q-learning/q-table.png"/>


接下来看看具体的代码实现 ⬇️
### 代码实现
```python
import numpy as np
import random

# 创建3x5迷宫环境
class MazeEnv:
    def __init__(self):
        # 定义迷宫：-1是可走路径，-100是障碍物，100是终点
        self.maze = np.array([
            [-1, -1, -1, -100, 100],
            [-1, -1, -100, -1, -1],
            [-1, -1, -1, -1, -1]
        ])
        self.start = (0, 0)  # 起点位置
        self.state = self.start
        self.actions = ['up', 'down', 'left', 'right']
    
    def reset(self):
        self.state = self.start
        return self.state
    
    def step(self, action):
        x, y = self.state
        if action == 'up':
            x = max(0, x - 1)
        elif action == 'down':
            x = min(2, x + 1)
        elif action == 'left':
            y = max(0, y - 1)
        elif action == 'right':
            y = min(4, y + 1)

        if self.maze[x, y] == -100:  # 遇到障碍物
            next_state = self.state  # 保持当前位置
        else:
            next_state = (x, y)

        reward = -1
        done = False
        if self.maze[x, y] == 100:  # 到达终点
            reward = 100
            done = True

        self.state = next_state
        return next_state, reward, done
    
    def is_terminal(self, state):
        x, y = state
        return self.maze[x, y] == 100

# 初始化 Q 表
Q_table = np.zeros((3, 5, 4))  # 3x5 的状态空间，4个动作

# 参数设置
learning_rate = 0.1  # 学习率
gamma = 0.9  # 折扣因子
epsilon = 0.1  # 探索率
episodes = 100  # 训练回合数

env = MazeEnv()

# 选择动作：ε-贪心策略
def choose_action(state):
    if np.random.uniform(0, 1) < epsilon:  # 探索
        action = np.random.choice(env.actions)
    else:  # 利用
        action_index = np.argmax(Q_table[state[0], state[1], :])
        action = env.actions[action_index]
    return action

# 训练过程
for episode in range(episodes):
    state = env.reset()
    done = False

    while not done:
        action = choose_action(state)
        next_state, reward, done = env.step(action)
        
        action_idx = env.actions.index(action)
        next_max = np.max(Q_table[next_state[0], next_state[1], :])
        
        # Q-learning 更新
        Q_table[state[0], state[1], action_idx] += learning_rate * (reward + gamma * next_max - Q_table[state[0], state[1], action_idx])
        
        state = next_state

# 输出训练后的 Q 表
for row in range(3):
    for col in range(5):
        if env.maze[row, col] == -100:
            print("####", end="  ")  # 障碍物
        elif env.maze[row, col] == 100:
            print(" G ", end="  ")  # 终点
        else:
            action_idx = np.argmax(Q_table[row, col, :])
            action = env.actions[action_idx]
            print(f" {action[0].upper()} ", end="  ")  # 输出最优动作（U,D,L,R）
    print()

```


上述示例，通过 **Q-learning** 算法，智能体不断进行探索和利用，最终学会在迷宫中找到最优路径。ε-贪心策略确保智能体在探索新路径和利用已有知识之间保持平衡。