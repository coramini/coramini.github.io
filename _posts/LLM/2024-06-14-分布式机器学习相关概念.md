---
layout: post
title: "分布式机器学习相关概念"
date: 2024-06-14
author: cora Liu
categories: [LLM]
---


分布式机器学习通过把任务分配给多个计算节点，让每个节点并行处理一部分工作，从而加速训练过程并处理大规模数据。

下面介绍分布式机器学习几种基本的通信方式 ⬇️

### 1、Broadcast
- 一对多广播。主节点将数据发送到其他节点，且数据内容相同。
- 数据并行的参数初始化，确保每张卡上的初始参数是一致的。

<img src="/assets/imgs/ai/llm/ZeRO/broadcast.png" width="400"/>

### 2、Scatter
- 一对多分散广播。主节点将数据发送到其他节点，且数据内容不相同。
- Reduce Scatter组合里的 Scatter操作。

<img src="/assets/imgs/ai/llm/ZeRO/scatter.png" width="400"/>

### 3、Gather
- 多对一。把多个节点的数据汇聚到一个节点上。

<img src="/assets/imgs/ai/llm/ZeRO/gather.png" width="400"/>

### 4、Reduce
- 多对一。把多个节点的数据规约运算到一个主节点上。

> 常用的规约操作符有：求累加和SUM、求累乘积PROD、求最大值MAX、求最小值MIN、逻辑与 LAND、按位与BAND、逻辑或LOR、按位或BOR、逻辑异或LXOR、按位异或BOXR、求最大值和最小大的位置MAXLOC、求最小值和最小值的位置MINLOC等。

<img src="/assets/imgs/ai/llm/ZeRO/reduce.png" width="400"/>

### 5、All Gather
- 多对多。收集所有的数据到所有的节点上。把多个节点的数据收集到一个主节点上（Gather），再把这个收集到的数据分发到其他节点上（broadcast）。

<img src="/assets/imgs/ai/llm/ZeRO/all-gather.png" width="400"/>

### 6、All Reduce
- 多对多。在所有节点上都执行相同的Reduce操作，将所有节点的数据规约运算得到的结果发送到所有的节点上。

<img src="/assets/imgs/ai/llm/ZeRO/all-reduce.png" width="400"/>

### 7、Reduce Scatter
- 多对多。在集群内的所有节点上都按维度执行相同的Reduce规约运算，再将结果发散到集群内所有的节点上。

<img src="/assets/imgs/ai/llm/ZeRO/reduce-scatter.png" width="400"/>

## DP vs DDP
在分布式机器学习中，`DDP（Distributed Data Parallel）`和 `DP（Data Parallel）`是两种常见的并行训练方法。它们的区别主要在于如何处理数据和模型，以及在多个计算节点之间如何进行通信。

### 1、PS (Parameter Server) 

参数服务器：一种分布式架构，参数服务器存储和管理模型参数，多个计算节点`（worker）并行训练模型，并定期与参数服务器同步参数更新。

### 2、Data Parallel (DP)

每个`GPU`处理一部分数据，并独立计算梯度。
所有`GPU`计算完成后，通过`All Reduce`操作同步梯度，然后更新模型参数。

<img src="/assets/imgs/ai/llm/ZeRO/dp.png" width="600"/>

**Forward**
- 复制模型参数，数据分片 • 每个GPU处理一部分

**Backward** 
- 梯度为每个卡的平均
- 通过 Parameter Server(PS, 一个内存更大的机器) 更新参数

### 3、Distributed Data Parallel (DDP)

每个`GPU`处理一部分数据，并独立计算梯度。
梯度同步和计算是重叠进行的，使用更高效的 `All Reduce` 操作同步梯度，然后更新模型参数。

<img src="/assets/imgs/ai/llm/ZeRO/ddp.png" width="400"/>


## 小结
总而言之，分布式机器学习可以理解为一种“团队合作”策略，目的是加速训练过程并处理大规模数据。