---
layout: post
title: "详解蒙特卡洛算法"
date: 2024-07-21
author: Cola Liu
categories: [LLM, RL]
---

蒙特卡洛方法是一种通过多次随机采样来估计系统某些特征（如期望值、概率等）的计算方法，广泛应用于强化学习和其他领域。

它的主要特点包括：

1. **随机采样**：通过反复进行随机试验或采样来近似计算期望值或概率。
2. **无需模型**：不需要了解系统的精确模型，只需通过采样得到的经验数据进行估计。
3. **适用于复杂问题**：可以应用于解析解困难或高维复杂问题的估计。
4. **渐进准确性**：随着采样次数增加，估计结果逐渐趋于真实值。
5. **依赖完整episode**：在强化学习中，通常需要一个完整episode的经验数据来更新值函数或策略。

这些特点使得蒙特卡洛方法在强化学习和数值计算中非常有用。

## 算法实现
**蒙特卡洛方法** 在强化学习中可以是**基于值（Value-based）** 也可以是**基于策略（Policy-based）**。具体取决于你在使用蒙特卡洛方法时如何组织你的算法。下面我会提供一个蒙特卡洛方法的代码示例，以迷宫游戏为例，解释它是如何作为基于值或基于策略的方法工作的。

### 基于值的蒙特卡洛方法

在基于值的蒙特卡洛方法中，我们通过评估状态或状态-动作对的价值来优化策略。它的基本步骤有：
- **choose action** 动作选择，结合**ε-greedy** 算法选择action
- **generate episode** 回合生成
- **update Q** 更新Q表

<img src="/assets/imgs/ai/llm/RL/montecarlo/montecarlo-value.png"/>

> 在同一episode中，智能体可能会多次访问相同的状态，并且在不同的时间步长上做出相同的动作。

例如，在迷宫游戏中，智能体可能由于走错路返回到之前的位置，并再次从该位置采取同样的动作。尽管状态和动作相同，但由于不同时间步长下未来的状态转移和奖励可能不同，因此未来回报也可能不同。

这里的代码示例演示了如何使用蒙特卡洛方法来估计 Q 值（状态-动作对的价值）。

#### 示例代码：基于值的蒙特卡洛方法

```python
import numpy as np
import random

# 迷宫定义（0 表示空地，1 表示墙壁，2 表示终点）
maze = np.array([
    [0, 0, 1, 0],
    [1, 0, 1, 0],
    [1, 0, 0, 0],
    [0, 1, 1, 2]
])

# 动作空间定义：上下左右
actions = ['up', 'down', 'left', 'right']

# Q值表初始化，初始Q值为0
Q = {}
for i in range(maze.shape[0]):
    for j in range(maze.shape[1]):
        if maze[i][j] == 0:  # 只对非墙壁区域初始化Q值
            Q[(i, j)] = {a: 0.0 for a in actions}

# 奖励函数，终点得到 +1 奖励，其他地方为 -0.04 惩罚
def get_reward(state):
    if maze[state] == 2:
        return 1.0
    return -0.04

# 通过贪心策略选择动作
def choose_action(state, epsilon=0.1):
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)  # 探索：随机选择动作
    else:
        q_values = Q[state]
        return max(q_values, key=q_values.get)  # 利用：选择Q值最大的动作

# 执行动作，返回新的状态
def take_action(state, action):
    i, j = state
    if action == 'up' and i > 0 and maze[i-1, j] != 1:
        return (i-1, j)
    elif action == 'down' and i < maze.shape[0]-1 and maze[i+1, j] != 1:
        return (i+1, j)
    elif action == 'left' and j > 0 and maze[i, j-1] != 1:
        return (i, j-1)
    elif action == 'right' and j < maze.shape[1]-1 and maze[i, j+1] != 1:
        return (i, j+1)
    return state  # 如果碰到墙壁，保持原地

# 生成一回合（episode）
def generate_episode(start_state, epsilon=0.1):
    episode = []
    state = start_state
    while True:
        action = choose_action(state, epsilon)
        new_state = take_action(state, action)
        reward = get_reward(new_state)
        episode.append((state, action, reward))
        if new_state == (3, 3):  # 到达终点
            break
        state = new_state
    return episode

# 更新Q值表，基于蒙特卡洛方法
def update_Q(episode, gamma=0.9):
    G = 0
    returns = {}
    for state, action, reward in reversed(episode):
        G = reward + gamma * G
        if (state, action) not in returns:  # 保证每个状态-动作对的首次访问更新
            returns[(state, action)] = G
            Q[state][action] += 0.1 * (G - Q[state][action])  # Q值更新公式

# 训练Q值表
def train_Q(start_state, episodes=1000, epsilon=0.1):
    for episode in range(episodes):
        episode_data = generate_episode(start_state, epsilon)
        update_Q(episode_data)

# 测试代码
start_state = (0, 0)  # 起点
train_Q(start_state, episodes=1000, epsilon=0.1)

# 打印最终的Q值表
for state in Q:
    print(f"State {state}: {Q[state]}")

```
### 基于策略的蒙特卡洛方法

在基于策略的蒙特卡洛方法中，我们直接优化策略。其基本步骤有：
- **choose action** 动作选择，概率分布选择action
- **generate episode** 回合生成
- **update policy** 更新策略

<img src="/assets/imgs/ai/llm/RL/montecarlo/montecarlo-policy.png"/>

以下示例展示了如何使用蒙特卡洛方法直接更新策略。
#### 示例代码：基于策略的蒙特卡洛方法

```python
import numpy as np
import random

# 迷宫定义（0 表示空地，1 表示墙壁，2 表示终点）
maze = np.array([
    [0, 0, 1, 0],
    [1, 0, 1, 0],
    [1, 0, 0, 0],
    [0, 1, 1, 2]
])

# 动作空间定义：上下左右
actions = ['up', 'down', 'left', 'right']

# 状态到动作的策略映射，初始化为均匀随机策略
policy = {}
for i in range(maze.shape[0]):
    for j in range(maze.shape[1]):
        if maze[i][j] == 0:  # 只对非墙壁区域初始化策略
            policy[(i, j)] = {a: 1/len(actions) for a in actions}

# 奖励函数，终点得到 +1 奖励，其他地方为 -0.04 惩罚
def get_reward(state):
    if maze[state] == 2:
        return 1.0
    return -0.04

# 通过当前策略选择动作
def choose_action(state):
    action_probs = list(policy[state].values())
    return np.random.choice(actions, p=action_probs)

# 执行动作，返回新的状态
def take_action(state, action):
    i, j = state
    if action == 'up' and i > 0 and maze[i-1, j] != 1:
        return (i-1, j)
    elif action == 'down' and i < maze.shape[0]-1 and maze[i+1, j] != 1:
        return (i+1, j)
    elif action == 'left' and j > 0 and maze[i, j-1] != 1:
        return (i, j-1)
    elif action == 'right' and j < maze.shape[1]-1 and maze[i, j+1] != 1:
        return (i, j+1)
    return state  # 如果碰到墙壁，保持原地

# 生成一回合（episode）
def generate_episode(start_state):
    episode = []
    state = start_state
    while True:
        action = choose_action(state)
        new_state = take_action(state, action)
        reward = get_reward(new_state)
        episode.append((state, action, reward))
        if new_state == (3, 3):  # 到达终点
            break
        state = new_state
    return episode

# 更新策略，基于蒙特卡洛方法
def update_policy(episode, gamma=0.9):
    G = 0
    returns = {}
    for state, action, reward in reversed(episode):
        G = reward + gamma * G
        if (state, action) not in returns:
            returns[(state, action)] = G
            total_prob = sum(policy[state].values())
            policy[state][action] += 0.1 * (G - policy[state][action])
            total_prob = sum(policy[state].values())
            for a in policy[state]:
                policy[state][a] /= total_prob

# 训练策略
def train_policy(start_state, episodes=1000):
    for episode in range(episodes):
        episode_data = generate_episode(start_state)
        update_policy(episode_data)

# 测试代码
start_state = (0, 0)  # 起点
train_policy(start_state, episodes=1000)

# 打印最终策略
for state in policy:
    print(f"State {state}: {policy[state]}")

```

### 区别总结

- 基于值的蒙特卡洛方法主要通过估计状态-动作对的Q值来改进策略。代理每次执行动作后，根据回报值更新Q值，并通过最大化Q值来选择动作。

- 基于策略的蒙特卡洛方法则直接优化策略的概率分布，而不需要显式估计Q值。




