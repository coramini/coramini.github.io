---
layout: post
title: "多层感知器MLP"
date: 2023-04-25
author: Cola Liu
categories: [深度学习]
---

# 多层感知器
MLP: Multilayer Perceptron

## 一、从非连续边界说起
现实生活中，有很多数据的边界并不是连续的，数据分布比较离散，数据的边界就是非连续的。

非连续边界数据是指在一定范围内不连续的数据点，举几个例子：

- 一个国家的人口分布数据，每个城市的人口数量是一个离散的数据点，城市之间存在空隙，属于非连续边界数据。
- 一个城市的交通拥堵指数数据，每个时段的指数是一个离散的数据点，不同时间段之间存在间隔，属于非连续边界数据。

以下就是一个非连续边界的数据分布 ⬇️

<img src="/assets/imgs/ai/MLP/非连续边界-数据.png" width="150" />

可以看到上图中的数据，**不能用一条直线或者一条连续的曲线去进行分类，只能用非连续的边界曲线去进行分类。**

为了了解**MLP**原理，我们把数据简化一下可以得到下图 ⬇️

<img src="/assets/imgs/ai/MLP/非连续边界-简化.png" width="400" />

用一个表格表示一下上图的值，可以看出这是一个**同或门**。**我们知道，同或门可以看成基础的与或非门等的组合。**

<img src="/assets/imgs/ai/MLP/非连续边界-同或.png" width="300" />

### 1、或门（OR）

> 或门是实现逻辑加的电路,又称逻辑和电路,简称或门。

<img src="/assets/imgs/ai/MLP/或.png" width="300" />

这是一个简单的**逻辑回归**分类问题。在这里我们选择一条简单的直线作为数据边界划分依据，可以得到 $ y = g(x_1, x_2) = \frac{1}{1+e^{-(x_1 + x_2  - 0.5)}} $

<img src="/assets/imgs/ai/MLP/或sigmoid.png" width="550" />

当然你也可以用其他的边界函数，因为这里的数据比较少，合适的边界函数有很多。

### 2、与门（AND）
接下来看看与门（AND）🫱 与门是实现逻辑“乘”运算的电路。

<img src="/assets/imgs/ai/MLP/与.png" width="300" />

同样，我们可以用 **逻辑回归** 模型去解决这个问题。可以得到：$ y = g(x_1, x_2) = \frac{1}{1+e^{-(x_1 + x_2  - 1.5)}} $

<img src="/assets/imgs/ai/MLP/与sigmoid.png" width="550" />

### 3、与非门（NAND）

最后看看与非门（NAND）：

<img src="/assets/imgs/ai/MLP/与非.png" width="300" />

同理可得，$ y = g(x_1, x_2) = \frac{1}{1+e^{-(-x_1 + -x_2 + 0.5)}} $

<img src="/assets/imgs/ai/MLP/与非sigmoid.png" width="550" />

### 4、同或门

最后，同或门其实就是上面或门、非门、与非门的组合。假设我们有 $ g(x) = \frac{1}{1+e^{-x}} $，那么

<img src="/assets/imgs/ai/MLP/同或-原理.png" width="500" />

用**matlab**把图画出来，可以得到一个非连续边界的结果图。

<img src="/assets/imgs/ai/MLP/同或sigmoid网格图.png" width="200" /> <img src="/assets/imgs/ai/MLP/同或sigmoid.png" width="230" />

参考代码如下 ⬇️
```python
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np
import pandas as pd
import matplotlib.ticker as ticker

def g(x):
    return 1 / (1 + pow(np.e, -x))

X1 = np.linspace(-1, 1, 10)
X2 = np.linspace(-1, 1, 10)

A1 = g(-10 * X1 - 10 * X2 + 5)
A2 = g(10 * X1 + 10 * X2 - 15)

X1_new, X2_new = np.meshgrid(X1, X2)
A1_new, A2_new = np.meshgrid(A1, A2)

Y = g(10 * A1_new + 10* A2_new - 5)

print(Y)
# Plot the surface
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
ax.set_xlim(-3,3)
ax.set_ylim(-3,3)
ax.set_title('x1 XNOR x2')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
ax.plot_wireframe(X1_new, X2_new, Y)

plt.show()

```

## 二、多层感知器 MLP
从上面例子我们知道，同或门就是多层简单的逻辑回归的嵌套。

<img src="/assets/imgs/ai/MLP/同或-原理.png" width="500" />

那么把这个模型再抽象一下，就可以得到多层感知器（Multilayer Perceptron，简称MLP），即多层简单回归嵌套的模型，它是神经网络的基础：

<img src="/assets/imgs/ai/MLP/MLP模型.png" width="400" />

> 多层感知器（Multilayer Perceptron，简称MLP）是一种人工神经网络模型，也是最常用的神经网络模型之一。它由一个或多个神经网络层组成，每一层由多个神经元（neuron）组成，每个神经元都与下一层的所有神经元相连。MLP可以用于分类、回归和预测等任务。

> MLP可以处理高维度、非线性和复杂的数据，如图像、音频和文本等。它具有良好的泛化能力，可以在训练集之外的数据上获得很好的性能。MLP也可以使用反向传播算法来进行训练，即通过计算误差梯度来更新模型参数，使模型可以适应不同的数据集和任务。

接下来我们来看看具体的计算过程。在这里我们选用的激活函数是**sigmoid函数** $ g(x) = \frac{1}{1+e^{-x}} $。
当然我们也可以选择其他的激活函数，如经常有所耳闻的**relu**函数。（在这里，我们暂且忽略常数项 $\theta_0$）

### 1、输入层计算过程


<img src="/assets/imgs/ai/MLP/MLP-a.png" width="500" />

### 2、中间层计算过程

<img src="/assets/imgs/ai/MLP/MLP-b.png" width="500" />

### 3、输出层计算过程

<img src="/assets/imgs/ai/MLP/MLP-y.png" width="500" />

具体的计算过程跟上面同或门的计算过程类似，在这里简单演示一下。
## 总结
总而言之，多层感知器 MLP，就是多层简单的回归（线性回归、逻辑回归等）的嵌套。