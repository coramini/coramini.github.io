---
layout: post
title: "mean函数的用法"
date: 2023-05-28
author: Cola Liu
categories: [编程篇, PyTorch]
---

在机器学习中，经常看到`tensor`的变换函数`mean`。先来看看`mean`函数的`api`定义。


```python
torch.mean(input, dim, keepdim=False, *, dtype=None, out=None) 
```
- input：输入矩阵
- dim：维度
- keepdim: 是否保留原来维度
- ...

所谓 `mean`，就是求**均值**的意思。

在`mean`函数中关键的参数是`dim`，`dim`可以理解为对该维度求均值。关于`dim`的取值比较理解起来比较抽象，接下来举例子🌰来说明。

## 二维tensor
首先，我们创建一个二维的 3x4 `tensor`。

```python
import torch
x = torch.linspace(1, 12, steps=12).view(3,4)
print(x)
```

打印的结果如下 ⬇️
```
tensor([[ 1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.],
        [ 9., 10., 11., 12.]])
```
 即这个张量有`3行4列`。在二维`tensor`中，`([3,4])`中的`3`对应`dim=0`，`4`对应的是`dim=1`。如下图所示 ⬇️

<img src="/assets/imgs/ai/PyTorch/mean/mean-1.png" width="200" />

### dim=0
接下来看看使用 `mean` 函数，当参数`dim=0`时， `tensor` 发生了什么变化。

```python
x = x.mean(dim=0)
print(x)
```
结果如下 ⬇️

```python
tensor([5., 6., 7., 8.]) 
""" 
跨行计算
5: 第一列 1、5、9 的平均值
6: 第二列 2、6、10 的平均值
7: 第三列 3、7、11 的平均值
8: 第四列 4、8、12 的平均值
"""
torch.Size([4])
```

我们用一张图来表示 ⬇️

<img src="/assets/imgs/ai/PyTorch/mean/mean-2.png" width="450" />

从上图可以看出，当 `dim=0` 时，我们基于该维度0（3行）, **跨行**计算平均值。

可以看成，对维度0上的值求均值（mean pooling）。这个时候，维度0就被压缩了，也就是我们通常说的“**降维**”。运算的结果就变成了**torch.Size([4])**。

### dim=1
同理，假设`dim=1`，那么我们基于维度1（4列），**跨列**计算平均值。

我们对维度1上的值求平均值（mean pooling）。这个时候，维度1就被压缩了，也就是我们通常说的“**降维**”。运算的结果就变成了**torch.Size([3])**。
<img src="/assets/imgs/ai/PyTorch/mean/mean-3.png" width="450" />

用代码验证一下

```python
x = x.mean(dim=1)
print(x)
print(x.shape)
```
打印结果如下 ⬇️
```python
tensor([ 2.5000,  6.5000, 10.5000])
""" 
跨列计算
2.5000: 第一行 1，2，3，4 的平均值
6.5000: 第二行 5，6，7，8 的平均值
10.5000: 第二行 8，9，10，11 的平均值
"""
torch.Size([3])
```

## 三维tensor
对于三维的`tensor`的计算也是一样的道理，假设我们有一个`5x3x4`的tensor：

```python
import torch
y = torch.linspace(1, 60, steps=60).view(5,3,4)
print(y)
print(y.shape)
```
结果如下 ⬇️
```python
tensor([[[ 1.,  2.,  3.,  4.],
         [ 5.,  6.,  7.,  8.],
         [ 9., 10., 11., 12.]],

        [[13., 14., 15., 16.],
         [17., 18., 19., 20.],
         [21., 22., 23., 24.]],

        [[25., 26., 27., 28.],
         [29., 30., 31., 32.],
         [33., 34., 35., 36.]],

        [[37., 38., 39., 40.],
         [41., 42., 43., 44.],
         [45., 46., 47., 48.]],

        [[49., 50., 51., 52.],
         [53., 54., 55., 56.],
         [57., 58., 59., 60.]]])
torch.Size([5, 3, 4])
```
 即这个向量有5层3行4列。在三维`tensor`中，`([5,3,4])`中的`5`对应`dim=0`，`3`对应的是`dim=1`，`4`对应的是`dim=2`，以此类推。
 接下来分别看看 dim=0,1,2 `tensor`发生了什么变化。

### dim=0
当参数`dim=0`时，我们有 ⬇️
```python
y = y.mean(dim=0)
print(y)
print(y.shape)
```
打印结果如下 ⬇️
```
tensor([[25., 26., 27., 28.],
        [29., 30., 31., 32.],
        [33., 34., 35., 36.]])
torch.Size([3, 4])
```

在这个例子中，`[5,3,4]`的 `tensor` 总共有5层3行4列，dim=0，即**跨层**计算平均值，所以计算结果是`torch.Size([3, 4])`。

<img src="/assets/imgs/ai/PyTorch/mean/mean-4.png" />

### dim=1
当参数`dim=1`时，我们有 ⬇️
```python
y = y.mean(dim=1)
print(y)
print(y.shape)
```
打印结果如下 ⬇️
```
tensor([[ 5.,  6.,  7.,  8.],
        [17., 18., 19., 20.],
        [29., 30., 31., 32.],
        [41., 42., 43., 44.],
        [53., 54., 55., 56.]])
torch.Size([5, 4])
```
在这个例子中，`[5,3,4]`的 `tensor` 总共有5层3行4列，`dim=1`，即**跨行**计算平均值，计算结果是`torch.Size([5, 4])`。
<img src="/assets/imgs/ai/PyTorch/mean/mean-5.png" />

### dim=2
当参数`dim=2`时，我们有 ⬇️
```python
y = y.mean(dim=2)
print(y)
print(y.shape)
```
打印结果如下 ⬇️
```
tensor([[ 2.5000,  6.5000, 10.5000],
        [14.5000, 18.5000, 22.5000],
        [26.5000, 30.5000, 34.5000],
        [38.5000, 42.5000, 46.5000],
        [50.5000, 54.5000, 58.5000]])
torch.Size([5, 3])
```
在这个例子中，`[5,3,4]`的 `tensor` 总共有5层3行4列，`dim=2`，即**跨列**计算平均值，计算结果是`torch.Size([5, 3])`。
<img src="/assets/imgs/ai/PyTorch/mean/mean-6.png" />

## 小结
从图可以直观看出，`mean` 函数的主要作用是进行降维处理，`dim` 的取值表示对该维度取均值。

一般来说，三维 `tensor` 通过 `mean` 计算之后会变成二维，二维 `tensor` 通过 `mean` 计算之后会变成一维。如果我们想保留原来的维度，可以使用 `keepDim=true`。