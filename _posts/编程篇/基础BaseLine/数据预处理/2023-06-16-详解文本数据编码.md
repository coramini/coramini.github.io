---
layout: post
title: "详解文本数据编码"
date: 2023-06-16
author: Cola Liu
categories: [编程篇,BaseLine,数据预处理,文本]
---

## 写在前面
本文主要介绍使用`SentencePiece`库进行机器学习中文本数据编码的详细过程。下面用一张图简单表示一下整个过程 ⬇️

<img src="/assets/imgs/ai/数据预处理/文本/sentence_encode.png" />

## SentencePieceTrainer 训练和生成模型
`SentencePieceTrainer`用于从未分词的原始文本数据中学习并生成分词模型，以便后续对文本进行编码和解码。


下面是一个输入和输出示例：

```python
import sentencepiece as spm

# 设置训练参数
input_file = "input.txt" # 该文件内容为:"你好，世界！"
model_prefix = "spm_model"
vocab_size = 10
model_type = "unigram"
shuffle_input_sentence=True
normalization_rule_name='nmt_nfkc_cf'

# 训练 SentencePiece 模型
spm.SentencePieceTrainer.train(input=f"{input_file}", model_prefix=model_prefix, vocab_size=vocab_size, model_type=model_type,shuffle_input_sentence=shuffle_input_sentence, normalization_rule_name=normalization_rule_name)

```
执行以上代码，程序会生成两个文件: `spm_model.model` 和 `spm_model.vocab`。
- `spm_model.model`保存训练好的model，该文件包括编码规则等等信息，它是一个二进制文件。

- `spm_model.vocab`是一个文本文件（.vocab），其中包含了子词单元及其对应的索引和频率信息。其内容如下 ⬇️

<img src="/assets/imgs/ai/数据预处理/文本/vocab.png" width="100"/>

## 模型训练参数

### 1、model_type
`SentencePiece` 的编码规则取决于所使用的模型类型`model_type`。常见的模型类型包括 **"unigram"**、**"bpe"（Byte Pair Encoding）** 和 **"char"（字符级别编码）** 等。每种模型类型都有不同的编码策略和规则。

#### Unigram 模型（"unigram"）
Unigram 模型将文本分割为最小的单位，如字符或子词。它通过学习出现频率来构建词汇表，出现频率高的单位将成为独立的标记。

`Unigram` 模型输入输出示例如下 ⬇️

`输入文本: "I love playing soccer."`
`输出编码结果: ['▁I', '▁love', '▁playing', '▁soccer', '.']`

在这个示例中，`Unigram` 模型将单词作为最小单位，其中以 "▁" 开头的部分表示单词的开头。每个单词被分割成独立的标记，标点符号保持不变。

#### BPE 模型（"bpe"）
`BPE` 模型将文本逐步合并成具有更高频率的子词或片段。它通过迭代地将出现频率高的相邻单位合并来构建词汇表。

`BPE` 模型输入输出示例：

`输入文本: "I like apples and oranges."`
`输出编码结果: ['I', 'Ġlike', 'Ġapp', 'les', 'Ġand', 'Ġorange', 's', '.']`

在这个示例中，`BPE` 模型将文本逐步合并成具有更高频率的子词或片段。每个子词都以 "Ġ" 开头，而出现频率高的子词被分割为独立的标记。

#### 字符级别模型（"char"）

字符级别模型将每个字符作为一个独立的标记。编码规则是将文本中的每个字符单独编码。

字符级别模型输入输出示例：

`输入文本: "Hello!"`
`输出编码结果: ['H', 'e', 'l', 'l', 'o', '!']`

在这个示例中，字符级别模型将每个字符作为一个独立的标记，没有对字符进行分割。


> 具体的编码规则会在训练过程中根据数据的统计信息和模型类型自动学习得出。因此，不同的模型类型和训练数据可能会导致不同的编码规则。你可以根据你的需求选择适合的模型类型，并根据实际数据进行训练和调整。


### 2、vocab_size

选择适当的 vocab_size 取决于你的具体应用场景、数据集的大小和语言特点等因素。

**小型数据集或资源受限**：10,000 或更少。这样可以减小模型的复杂性和内存占用，并加快训练和推理的速度。

> 需要注意，较小的 vocab_size 可能会导致词汇表覆盖不足的问题，特别是对于包含大量专业术语或罕见单词的领域。

**中等规模的数据集和一般应用场景**： 10,000 到 50,000 之间。这样可以平衡模型的复杂性和覆盖范围，能够处理常见的词汇和一些特殊的词汇。

**大型数据集和需要处理丰富语言细节的任务**： 50,000 到 100,000 或更多。这样可以更好地捕捉到更多的词汇和语言结构，提高模型的表现。

> 需要注意的是，较大的 vocab_size 会增加模型的复杂性和计算开销。

根据你的具体需求和可用资源，尝试不同的 vocab_size 值，并根据模型的性能、训练速度和资源限制进行评估和调整。


### 3、charater_coverage

`character_coverage` 参数用于控制 `SentencePiece` 模型中覆盖的字符比例。它指定了模型应该覆盖原始文本中的多少个字符。

具体来说，`character_coverage` 参数的取值范围是`0到1之间的小数`，表示模型应覆盖的字符比例。

较高的值会使模型覆盖更多的字符，而较低的值会限制模型只覆盖常见字符。

- **较高的值**：对于包含广泛字符集的多语言数据集，通常需要将 `character_coverage` 设置为较高的值，例如`0.9995或更接近1`的值。这样可以确保模型能够覆盖多种语言中的大部分字符。

- **较低的值**：对于仅涉及特定字符范围（例如英文字母、数字等）的任务，可以将 `character_coverage` 设置为较低的值，例如`0.5或更接近0`的值。这样可以限制模型仅关注于特定字符范围内的标记。

> 需要注意的是，character_coverage 的选择应该综合考虑词汇表的大小、模型的性能和计算资源的限制。过低的值可能导致模型无法覆盖重要的字符，而过高的值可能增加模型的复杂性和训练开销。

### 4、shuffle_input_sentence


- `shuffle_input_sentence` 设置为 `True`：训练器会在训练过程中对输入的句子进行随机排序。这可以帮助增加训练数据的多样性，减少模型对句子顺序的依赖性。通过随机排序句子，模型可以更好地学习句子之间的上下文和关联。

- `shuffle_input_sentence` 设置为 `False`：训练器将按照输入句子的顺序进行训练。这在某些情况下可能有用，例如对于特定顺序的文本数据，或者在需要保留原始文本顺序信息的任务中。



接下来，我们用训练好的模型`model`对文件数据进行编码。 `spm_model.encode` 用于对文本进行编码，通常是基于预训练的语言模型或分词器。

## SentencePieceProcessor 数据编码

在进行文本数据编码的之前，需要把之前训练好的模型model加载出来。model记录了编排词汇表的详细规则，我们是基于原来训练好的编排好的词汇表进行数据编码。

```python
# 加载训练好的 SentencePiece 模型
spm_model = spm.SentencePieceProcessor()
spm_model.load(f"{model_prefix}.model")

# 要编码的文本
text = "你好，世界！"

# 对文本进行编码
encoded_text = spm_model.encode(text)

# 打印编码后的结果
print(encoded_text) # [5, 7, 8, 4, 6, 9, 3]
```
可以看到编码后的数据是一个数组。数组中每个元素对应该字符在词汇表`vocab`中的位置信息。完整的流程如下图所示 ⬇️

<img src="/assets/imgs/ai/数据预处理/文本/sentence_encode_more.png" />

如果我们想依旧输出字符串，那么 `encode` 函数加上 `out_type` 参数即可：
```
encoded_text = spm_model.encode(txt, out_type=str)
```


## 小结

由于计算机只能处理数字数据，所以我们要进行文本数据编码。文本数据是以字符和字符串的形式存在的，而计算机处理数据时通常需要将其表示为数字形式。

通过文本数据编码，我们得到了一个词汇表，并且输出了文本文件每个句子中的每个分词（单词、子词或字符）在词汇表的位置信息数组。



