---
layout: post
title: "文本数据预处理基本流程"
date: 2023-06-16
author: cora Liu
categories: [编程篇,BaseLine,数据预处理,文本]
---

## 写在前面
在机器学习中，经常会遇到输入为文本数据的任务，本文主要介绍中英文翻译任务中文本数据预处理的基本流程。

<img src="/assets/imgs/ai/数据预处理/文本/data_process.png" />
## 初始输入
本文初始输入为：
- `train_dev.raw.en` 训练数据，英文。
- `train_dev.raw.zh`训练数据，中文，与`train_dev.raw.en`一一对应。
- `test.raw.en` 测试数据，英文。
- `test.raw.zh` 测试数据，中文，与`test.raw.en`一一对应。

其中每个文件为简单的文本文件。

<img src="/assets/imgs/ai/数据预处理/文本/input.png" />


## 一、文本清洗 clean_corpus


**clean_corpus**（清理语料库）是一个常见的文本预处理步骤，用于清理和规范化文本数据。它通常涉及以下一些操作：

- **去除特殊字符和标点符号**： 这一步骤通常包括去除文本中的标点符号、特殊字符和符号。这样做可以消除干扰和噪声，并使文本更具可读性。

- **转换为小写字母**： 将文本中的所有字母转换为小写形式。这样做可以消除大小写带来的差异，使模型更好地学习文本的内容。

- **去除停用词**： 停用词是在文本处理中通常被忽略的常见词语，如“the”、“a”、“is”等。去除这些词语可以减少数据维度和噪声，并提高模型的效果。

- **词干化或词形还原**： 这一步骤旨在将单词还原为其基本形式，例如将动词的不同形式还原为原始动词，如“running”还原为“run”。这可以减少词形的变化带来的词汇量增加，并将相关词汇归并到一起。

- **去除数字**： 如果任务不需要数字信息，可以将文本中的数字去除。这有助于减少数据中的噪声和维度。

`clean_corpus` 可以帮助模型更好地理解文本数据，并提高后续NLP任务（如文本分类、情感分析、机器翻译等）的性能和效果。预处理过程可能因任务的特定要求而有所不同，上述步骤是常见的预处理步骤之一。

以下为一些常用操作的代码示例 ⬇️

### 1、中文全角字符处理

如果训练数据有中文材料的话，还需要把全角字符转换成半角字符，如下列`strQ2B`函数所示 ⬇️
```python
def strQ2B(ustring):
    """Full width -> half width"""
    # reference:https://ithelp.ithome.com.tw/articles/10233122
    ss = []
    for s in ustring:
        rstring = ""
        for uchar in s:
            inside_code = ord(uchar)
            if inside_code == 12288:  # Full width space: direct conversion
                inside_code = 32
            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion
                inside_code -= 65248
            rstring += chr(inside_code)
        ss.append(rstring)
    return ''.join(ss)
```

### 2、标点符号/特殊字符处理
本文主要介绍的中英文翻译任务中的文本数据预处理，所以需要处理不同的标点符号，以及某些特殊的字符。

```python
def clean_s(s, lang):
    if lang == 'en':
        s = re.sub(r"$[^()]*$", "", s) # remove ([text])
        s = s.replace('-', '') # remove '-'
        s = re.sub('([.,;!?()\"])', r' \1 ', s) # keep punctuation
    elif lang == 'zh':
        s = strQ2B(s) # Q2B
        s = re.sub(r"$[^()]*$", "", s) # remove ([text])
        s = s.replace(' ', '')
        s = s.replace('—', '')
        s = s.replace('“', '"')
        s = s.replace('”', '"')
        s = s.replace('_', '')
        s = re.sub('([。,;!?()\"~「」])', r' \1 ', s) # keep punctuation
    s = ' '.join(s.strip().split())
    return s
```

### 3、长短句处理
为了保证训练结果良好，我们通常需要对训练数据做出相应的处理，如剔除过长或者过短的句子。如果中英文对照句子长度差异过大，也应该被剔除。

```python
def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):
    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():
        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')
        return
    with open(f'{prefix}.{l1}', 'r') as l1_in_f:
        with open(f'{prefix}.{l2}', 'r') as l2_in_f:
            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:
                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:
                    for s1 in l1_in_f:
                        s1 = s1.strip()
                        s2 = l2_in_f.readline().strip()
                        s1 = clean_s(s1, l1)
                        s2 = clean_s(s2, l2)
                        s1_len = len_s(s1, l1)
                        s2_len = len_s(s2, l2)
                        if min_len > 0: # remove short sentence
                            if s1_len < min_len or s2_len < min_len:
                                continue
                        if max_len > 0: # remove long sentence
                            if s1_len > max_len or s2_len > max_len:
                                continue
                        if ratio > 0: # remove by ratio of length
                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:
                                continue
                        print(s1, file=l1_out_f)
                        print(s2, file=l2_out_f)
```
在上面代码中，我们把中英文对照的文件同时打开，逐行逐句进行处理。首先利用`clean_s`进行全角半角字符处理以及标点符号/特殊字符处理。

接着对输入的文本进行**长短句处理**，剔除过短过长的句子，以及中英文长度差异过大的句子，在这里做了一次简单数据清理工作。


## 二、数据分离
在上述例子中，我们需要把数据分离为`训练数据`和`验证数据`。
首先，固定我们的 **random seed（随机种子）**，这里可以保证每次生成的随机序列是一致的。

```python
seed = 33
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  
np.random.seed(seed)  
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
```

接着，设定分离比率`ratio`。这里为 0.99 / 0.01。
```python
valid_ratio = 0.01 # 3000~4000 would suffice
train_ratio = 1 - valid_ratio
```

开始进行数据分离 ⬇️

```python
if (prefix/f'train.clean.{src_lang}').exists() \
and (prefix/f'train.clean.{tgt_lang}').exists() \
and (prefix/f'valid.clean.{src_lang}').exists() \
and (prefix/f'valid.clean.{tgt_lang}').exists():
    print(f'train/valid splits exists. skipping split.')
else:
    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))
    labels = list(range(line_num))
    random.shuffle(labels)
    for lang in [src_lang, tgt_lang]:
        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')
        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')
        count = 0
        for line in open(f'{data_prefix}.clean.{lang}', 'r'):
            if labels[count]/line_num < train_ratio:
                train_f.write(line)
            else:
                valid_f.write(line)
            count += 1
        train_f.close()
        valid_f.close()
```

## 三、数据编码

因为计算机只能处理数字数据，所以我们要进行文本数据编码。文本数据是以字符和字符串的形式存在的，而计算机处理数据时通常需要将其表示为数字形式。

在这里利用 `sentencepiece`包进行文本数据编码的相关处理。

<img src="/assets/imgs/ai/数据预处理/文本/sentence_encode.png" />

### 1、train
这里利用`spm.SentencePieceTrainer.train` 训练以及生成模型。训练完成后，`spm.SentencePieceTrainer.train` 将生成一个包含学习到的子词分割模型的文件。

该模型可以存储为二进制文件（.model）或文本文件（.vocab），其中包含了子词单元及其对应的**索引**和**频率**信息。

```python
import sentencepiece as spm
vocab_size = 8000
if (prefix/f'spm{vocab_size}.model').exists():
    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')
else:
    spm.SentencePieceTrainer.train(
        input=','.join([f'{prefix}/train.clean.{src_lang}',
                        f'{prefix}/valid.clean.{src_lang}',
                        f'{prefix}/train.clean.{tgt_lang}',
                        f'{prefix}/valid.clean.{tgt_lang}']),
        model_prefix=prefix/f'spm{vocab_size}',
        vocab_size=vocab_size,
        character_coverage=1,
        model_type='unigram', # 'bpe' works as well
        input_sentence_size=1e6,
        shuffle_input_sentence=True,
        normalization_rule_name='nmt_nfkc_cf',
    )
```

### 2、encode
通过使用 `spm_model.encode` 方法，可以将输入的文本数据转换为 `SentencePiece` 模型所定义的子词单元序列，以便进行后续的文本处理和自然语言处理任务。

在这里需要`load`前面训练好的`model`。

```python
spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))
in_tag = {
    'train': 'train.clean',
    'valid': 'valid.clean',
    'test': 'test.raw.clean',
}
for split in ['train', 'valid', 'test']:
    for lang in [src_lang, tgt_lang]:
        out_path = prefix/f'{split}.{lang}'
        if out_path.exists():
            print(f"{out_path} exists. skipping spm_encode.")
        else:
            with open(prefix/f'{split}.{lang}', 'w') as out_f:
                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:
                    for line in in_f:
                        line = line.strip()
                        tok = spm_model.encode(line, out_type=str)
                        print(' '.join(tok), file=out_f)
```


## 四、转为二进制 binarization
文本编码后要转为二进制的文件，将文本编码后转换为二进制文件可以提供更高的存储效率、加载速度和数据保护，并增加平台的兼容性。

这对于处理大规模文本数据集和保护数据的完整性非常重要。

在这里使用`fairseq`包帮助处理 ⬇️

```python
binpath = Path('./DATA/data-bin', dataset_name)
if binpath.exists():
    print(binpath, "exists, will not overwrite!")
else:
    !python -m fairseq_cli.preprocess \
        --source-lang {src_lang}\
        --target-lang {tgt_lang}\
        --trainpref {prefix/'train'}\
        --validpref {prefix/'valid'}\
        --testpref {prefix/'test'}\
        --destdir {binpath}\
        --joined-dictionary\
        --workers 2
```

### joined-dictionary共享词典

`Joined-dictionary（合并词典）`是一种在多个语言或文本数据集中共享相同词汇的词典表示方式。

在机器翻译和多语言处理任务中，通常会遇到多个语言之间的对应关系和共享词汇。为了有效地处理这些情况，可以使用一个合并词典来表示这些语言的共享词汇。

当使用共享词典进行机器翻译时，下面是一个简单的输入输出例子：

输入：
源语言句子（英语）: "I love cats."
目标语言句子（法语）: "J'aime les chats."

输出：
共享词典的词汇表（仅示例，实际词汇表可能更大）：

```
['<pad>', '<unk>', '<s>', '</s>', 'I', 'love', 'cats', '.', 'J\'', 'aime', 'les', 'chats']
```
源语言句子经过共享词典编码后的结果：

```
[4, 5, 6, 7]
```
目标语言句子经过共享词典编码后的结果：

```
[8, 9, 10, 11, 7]
```
在这个例子中，源语言和目标语言共享同一个词汇表，源语言和目标语言的句子都可以在同一个嵌入向量空间中进行表示和处理。

## 小结
不同的数据输入类型有不同的数据处理流程，我们需要分析具体的场景，采用合适的数据处理方法。