---
layout: post
title: "音频数据预处理(上)"
date: 2023-05-22
author: cora Liu
categories: [编程篇,BaseLine,数据预处理,音频]
---

## 问题分类

对数据进行预处理，首先需要对待解决的问题进行分类。通过问题分类，找到合适的数据预处理的方法。

那么，跟语音相关的机器学习方向主要有以下的几种：

- 语音翻译：旨在将语音信号转换为文本。它们可以用于语音助手、语音命令识别和语音转写等应用。（speech => text）

- 语音合成：利用机器学习算法将文本转换为自然流畅的语音。它们可以应用于自动助手、有声读物和语音辅助技术等领域。 (text => speech)

- 声音识别：这些项目旨在根据说话人的声音特征对其进行身份识别。它们可以用于语音认证、个性化用户体验和电话识别等场景。（分类问题）

- 情感分析：这些项目旨在对不同类型的声音进行分类和分析。例如，识别环境声音、音乐风格分类、情感分析等。（分类问题）

接下来，介绍一下【声音识别】的音频数据处理简要流程～

## 问题分析

声音识别问题是一个多分类问题。属于 `supervised Learning` 的范畴。
<img src="/assets/imgs/ai/监督学习.png" />

supervised learning 的特点是，每一个 data 都打上了 label，也就是每一条数据都做好了标记，标记声音是属于哪个人的。

那么，对于 supervised learning，我们需要把每一条数据都处理成以下这种格式：

> data, label

可以把它想象成表格中的一行数据，则有一列是data，另一列是label。

同理可得，如果我们有多个数据（batch）那么处理的目标就是：

- data1, label1
- data2, label2
- data3, label3
- data4, label4
- ...
- datan, labeln

## data 和 label

从上面的分析可以得到，我们需要把数据处理成 `（data, label）` 格式。问题来了，`data` 和 `label` 是什么呢？

在音频数据中，`data` 是音频源，即读取音频文件中的内容。 `label` 是每个speaker唯一的身份标识，可以用id来表示。

## 举个栗子 🌰

下面是一份音频pt文件 ⬇️

<img src="/assets/imgs/ai/数据预处理/音频/dataset.png" />

- uttr-0a0ac1a3b4784aba9f76c1c7e6e920b5.pt
- uttr-0a18eb1376e141be814dc1bea80a39c0.pt
- ...
就是音频的pt文件（经过Pytorch预处理的文件），我们需要把它们读取出来（有几个G的.pt文件），这就是 `data`。
`label`有时候是在音频pt文件命名的时候顺便做好标记，在这里就不一样，`label`与音频pt文件`data`之间的关系记录在`mapping.json\metadata.json`里面。即用一个`json`文件去维护`data`与`label`之间的映射关系。

接下来分别来看看这几个音频文件：

### 1、mapping.json

<img src="/assets/imgs/ai/数据预处理/音频/mapping.png" style="display:block;" />

也就是说，`mapping.json` 维护的是`speaker`与`id`的映射关系。这里的`id`就是我们想要的`label`了。

<img src="/assets/imgs/ai/数据预处理/音频/speaker2id.png" />

### 2、metadata.json

<img src="/assets/imgs/ai/数据预处理/音频/metadata.png" style="display:block;" />

`metadata.json` 维护的是`speaker`与`feature_path`的映射关系,也就是音频pt文件的地址的映射关系。读取 `feature_path` 中的数据就可以得到 `data`。

<img src="/assets/imgs/ai/数据预处理/音频/speaker2path.png" style="display:block;" />

### 3、读取音频pt文件并裁减
我们知道，通常一份音频有长有短，这时候我们需要对音频`mel`进行剪辑，让它们之间的len保持一致。在这里每段音频截取128的length。

<img src="/assets/imgs/ai/数据预处理/音频/melcut.png" />

最终我们利用`mapping.json`和`metadata.json`把 `mel` 和 `id` map起来，作为`data`和`label`

<img src="/assets/imgs/ai/数据预处理/音频/mel2id.png" />

## Pytorch实现

```python

import os
import json
import torch
import random
from pathlib import Path
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
 
 
class myDataset(Dataset):
 def __init__(self, data_dir, segment_len=128):
  self.data_dir = data_dir
  self.segment_len = segment_len
 
  # Load the mapping from speaker neme to their corresponding id. 
  mapping_path = Path(data_dir) / "mapping.json"
  mapping = json.load(mapping_path.open())
  self.speaker2id = mapping["speaker2id"]
 
  # Load metadata of training data.
  metadata_path = Path(data_dir) / "metadata.json"
  metadata = json.load(open(metadata_path))["speakers"]
 
  # Get the total number of speaker.
  self.speaker_num = len(metadata.keys())
  self.data = []
  for speaker in metadata.keys():
   for utterances in metadata[speaker]:
    self.data.append([utterances["feature_path"], self.speaker2id[speaker]])
 
 def __len__(self):
   return len(self.data)
 
 def __getitem__(self, index):
  feat_path, speaker = self.data[index]
  # Load preprocessed mel-spectrogram.
  mel = torch.load(os.path.join(self.data_dir, feat_path))

  # Segmemt mel-spectrogram into "segment_len" frames.
  if len(mel) > self.segment_len:
   # Randomly get the starting point of the segment.
   start = random.randint(0, len(mel) - self.segment_len)
   # Get a segment with "segment_len" frames.
   mel = torch.FloatTensor(mel[start:start+self.segment_len])
  else:
   mel = torch.FloatTensor(mel)
  # Turn the speaker id into long for computing loss later.
  speaker = torch.FloatTensor([speaker]).long()
  return mel, speaker
 
 def get_speaker_number(self):
  return self.speaker_num

```
