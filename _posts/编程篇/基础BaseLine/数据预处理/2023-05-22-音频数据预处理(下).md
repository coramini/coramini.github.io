---
layout: post
title: "音频数据预处理(下)"
date: 2023-05-22
author: cora Liu
categories: [编程篇,BaseLine,数据预处理,音频]
---


## 批量数据处理——batch

一般来说，我们从 Dataset 的 `__getitem__` 中获取到的数据是一个一个的。

也即是以下这种形式 ⬇️

- data, label
- data, label
- data, label
- ...

在这里，`data` 和 `label` 分别对应是音频文件 `mel` 和 speaker的`id`。

- mel, id
- mel, id
- mel, id
- ...

那么拿到单条数据之后，我们需要对数据进行**批量处理**。

- 首先，把数据用 `Zip` 进行处理，把 `mel` 和 `id` 分别作为单独的一列。（可以理解为放在一张表格中，一列放 `mel`，一列放 `id`）

    |  mel | id |
    |--|--|
    |mel1| 1|
    |mel2| 2|
    |mel3| 3|
    |...| ...|

```python
mel, speaker = zip(*batch)
```

- `mel` 填充
也许你会问，我们上一节不是都处理成长度128了吗？那这里又是填充什么呢。这里填充的是mel的深度，让每一条数据的大小都为[128, 40]。这里用到的是`pytorch` 中的 `padding_squence`方法。

<img src="/assets/imgs/ai/数据预处理/音频/batch_process.png" />

```python
 mel = pad_sequence(mel, batch_first=True, padding_value=-20)
```

## 数据分离

拿到处理完的数据后，通常我们需要对数据进行分离。把数据集分为 **训练集trainset** 和 **验证集validset**。

PyTorch 为我们提供好了一个🪓可以把数据“劈开”，即 `torch.utils.data` 中的`random_split`。

<img src="/assets/imgs/ai/数据预处理/音频/random_split.png" />

在这里，我们假设`trainset`占了90%，`validset`占10% ⬇️

```python
 # Split dataset into training dataset and validation dataset
 trainlen = int(0.9 * len(dataset))
 lengths = [trainlen, len(dataset) - trainlen]
 trainset, validset = random_split(dataset, lengths)
```

## DataLoader

在这里，我们用`PyTorch` 中的 `DataLoader`来做数据批量处理的工作。`DataLoader`是一个数据加载相关的类，我们把上面的 batch数据处理函数，以及 `trainset`、`validset` 作为参数传入。

<img src="/assets/imgs/ai/数据预处理/音频/dataloader.png" />

```python
 # train_loader
 train_loader = DataLoader(
  trainset,
  batch_size=batch_size,
  shuffle=True,
  drop_last=True,
  num_workers=n_workers,
  pin_memory=True,
  collate_fn=collate_batch,
 )

 # valid_loader
 valid_loader = DataLoader(
  validset,
  batch_size=batch_size,
  num_workers=n_workers,
  drop_last=True,
  pin_memory=True,
  collate_fn=collate_batch,
 )
```

<img src="/assets/imgs/ai/数据预处理/音频/collate_fn.png" style="display:block;"/>



从上图可以看到， 批量处理数据函数`collate_batch`，作为`DataLoader`的`collate_fn` 参数传入。

总的代码实现如下 ⬇️

```python
import torch
from torch.utils.data import DataLoader, random_split
from torch.nn.utils.rnn import pad_sequence


def collate_batch(batch):
 # Process features within a batch.
 """Collate a batch of data."""
 mel, speaker = zip(*batch)
 # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.
 mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.
 # mel: (batch size, length, 40)
 return mel, torch.FloatTensor(speaker).long()


def get_dataloader(data_dir, batch_size, n_workers):
 """Generate dataloader"""
 dataset = myDataset(data_dir)
 speaker_num = dataset.get_speaker_number()
 # Split dataset into training dataset and validation dataset
 trainlen = int(0.9 * len(dataset))
 lengths = [trainlen, len(dataset) - trainlen]
 trainset, validset = random_split(dataset, lengths)

 train_loader = DataLoader(
  trainset,
  batch_size=batch_size,
  shuffle=True,
  drop_last=True,
  num_workers=n_workers,
  pin_memory=True,
  collate_fn=collate_batch,
 )
 valid_loader = DataLoader(
  validset,
  batch_size=batch_size,
  num_workers=n_workers,
  drop_last=True,
  pin_memory=True,
  collate_fn=collate_batch,
 )

 return train_loader, valid_loader, speaker_num
```
