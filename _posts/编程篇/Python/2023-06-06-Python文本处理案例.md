---
layout: post
title: "Python文本处理案例"
date: 2023-06-06
author: cora Liu
categories: [编程篇, Python]
---

Python 文本处理是机器学习常见的操作。本文整理了几种常见的文本处理操作。

## 1、文本切片

在`Python`中，你可以使用`切片（slicing）`操作来截取文本中固定长度的字符串。切片操作使用方括号和冒号来指定要提取的部分。下面是一个示例：

```python
text = "今天吃什么？"
fixed_length = 4
substring = text[:fixed_length]  # 从开头截取固定长度的字符串
print(substring)  # 输出："今天吃什"
```

在上面的示例中，`text`是要截取的文本，`fixed_length`是你想要的固定长度。通过使用切片操作`text[:fixed_length]`，你可以从开头截取固定长度的字符串。

如果你想从文本中的特定位置开始截取，可以使用类似的切片操作。例如，如果你想从第3个字符开始截取4个字符，可以这样做：

```python
text = "今天晚上吃什么呀"
start_index = 2  # 第三个字符的索引是2
fixed_length = 4
substring = text[start_index:start_index+fixed_length]  # 从特定位置截取固定长度的字符串
print(substring)  # 输出："晚上吃什"
```

在上面的示例中，`start_index`表示要开始截取的位置的索引。通过使用切片操作`text[start_index:start_index+fixed_length]`，你可以从特定位置开始截取固定长度的字符串。

## 2、获取特定位置字符

要获取字符串中特定位置的字符，可以使用索引操作符`[]`和该字符在字符串中的索引位置。
以下是一个示例：

```python
text = "Hello, World!"
first_character = text[0]  # 获取第一个字符
print(first_character)  # 输出：H

third_character = text[2]  # 获取第三个字符
print(third_character)  # 输出：l

last_character = text[-1]  # 获取最后一个字符
print(last_character)  # 输出：!
```

在上面的示例中，我们使用索引操作符`[]`和字符在字符串中的索引位置来获取特定位置的字符。`text[0]`表示获取字符串的第一个字符，`text[2]`表示获取字符串的第三个字符，`text[-1]`表示获取字符串的最后一个字符。

需要注意的是，如果你尝试访问超出字符串长度范围的索引位置，将会引发`IndexError`异常。因此，确保索引值在合法范围内。


## 3、提取pdf内容
要在`Python`中提取`PDF`的内容，你可以使用第三方库如`PyPDF2`或`pdfminer.six`。这些库提供了处理`PDF`文件的功能。

下面是使用`PyPDF2`库提取`PDF`内容的示例代码：

``` python
# pip install PyPDF2  安装 PyPDF2

import PyPDF2
from PyPDF2 import PdfFileReader

# Creating a pdf file object

pdf = open("test.pdf", "rb")

# Creating pdf reader object

pdf_reader = PyPDF2.PdfFileReader(pdf)

# Checking total number of pages in a pdf file

print("Total number of Pages:", pdf_reader.numPages)

# Creating a page object

page = pdf_reader.getPage(200)

# Extract data from a specific page number

print(page.extractText())

# Closing the object

pdf.close()

```

在上面的代码中，我们首先使用`open`函数打开PDF文件，并以二进制模式`（'rb'）`读取文件。然后，我们创建了一个`PdfFileReader`对象`pdf_reader`，用于读取和解析`PDF`文件。
我们使用`numPages`属性获取`PDF`文件的总页数，并通过循环逐页提取内容。使用`getPage`方法可以获取指定页码的`Page`对象，然后使用`extract_text`方法提取该页的文本内容。

## 4、提取word内容

要在`Python`中提取`Word`文档的内容，可以使用`Python-docx`库。`Python-docx`是一个第三方库，用于读取、修改和创建`Microsoft Word`文档（`.docx`文件）。

以下是使用`Python-docx`库提取`Word`文档内容的示例代码：

```python
# pip install python-docx  安装 python-docx

import docx

def main():
    try:
        doc = docx.Document('test.docx')  # Creating word reader object.
        data = ""
        fullText = []
        for para in doc.paragraphs:
            fullText.append(para.text)
            data = '\n'.join(fullText)

        print(data)
 
    except IOError:
        print('There was an error opening the file!')
        return

if __name__ == '__main__':
    main()

```
在上面的示例代码中，我们首先使用`Document`类打开`Word`文档，并将其赋值给`doc`变量。然后，我们使用`paragraphs`属性提取文档中的段落内容。通过遍历`paragraphs`列表，我们可以获取每个段落的文本内容。

此外，我们还可以使用tables属性提取文档中的表格内容。通过遍历`tables`列表、行和单元格，我们可以获取每个单元格的文本内容。


## 5、提取web网页内容
要在`Python`中提取`Web`网页的内容，你可以使用第三方库如`Requests`和`BeautifulSoup`。`Requests`库用于发送`HTTP`请求并获取网页内容，而`BeautifulSoup`库用于解析和提取`HTML`或`XML`文档中的数据。

以下是一个使用`Requests`和`BeautifulSoup`库提取`Web`网页内容的示例代码：

```python
# pip install bs4  安装 bs4

from urllib.request import Request, urlopen
from bs4 import BeautifulSoup

req = Request('<http://www.cmegroup.com/trading/products/#sortField=oi&sortAsc=false&venues=3&page=1&cleared=1&group=1>',
              headers={'User-Agent': 'Mozilla/5.0'})

webpage = urlopen(req).read()

# Parsing

soup = BeautifulSoup(webpage, 'html.parser')

# Formating the parsed html file

strhtm = soup.prettify()

# Print first 500 lines

print(strhtm[:500])

# Extract meta tag value

print(soup.title.string)
print(soup.find('meta', attrs={'property':'og:description'}))

# Extract anchor tag value

for x in soup.find_all('a'):
    print(x.string)

# Extract Paragraph tag value

for x in soup.find_all('p'):
    print(x.text)
```
在上述示例中，我们首先使用`Requests`库发送`HTTP`请求并获取网页的内容。需要替换`url`变量为你想要提取内容的网页`URL`。

接下来，我们使用`BeautifulSoup`库解析网页内容。在这个示例中，我们使用`html.parser`解析器来解析HTML文档，但你可以根据需要选择其他解析器。

然后，我们可以使用`BeautifulSoup`提供的各种方法和选择器来提取特定的元素或数据。在示例中，我们提取了网页的链接（`a`标签）和所有的段落元素（`p`标签）。

最后，我们处理提取到的数据并进行打印。

## 6、读取json内容

在`Python`中读取`JSON`内容很简单，可以使用内置的`json`模块。下面是一个示例代码，展示了如何读取`JSON`文件和解析`JSON`字符串：

```python
import json

# 读取JSON文件
with open('data.json') as file:
    data = json.load(file)
    # 使用data变量访问JSON数据

# 解析JSON字符串
json_string = '{"name": "John", "age": 30, "city": "New York"}'
data = json.loads(json_string)
print(data) # {'name': 'John', 'age': 30, 'city': 'New York'}
# 使用data变量访问JSON数据
```

## 7、删除字符串中的标点符号
要删除字符串中的标点符号，可以使用`Python`中的正则表达式和字符串翻译。
```python
import re
import string

data = "Stuning even for the non-gamer: This sound track was beautiful!\
It paints the senery in your mind so well I would recomend\
it even to people who hate vid. game music! I have played the game Chrono \
Cross but out of all of the games I have ever played it has the best music! \
It backs away from crude keyboarding and takes a fresher step with grate\
guitars and soulful orchestras.\
It would impress anyone who cares to listen!"

# Methood 1 : Regex

# Remove the special charaters from the read string

no_specials_string = re.sub('[!#?,.:";]', '', data)
print(no_specials_string)

# Methood 2 : translate()

# Rake translator object

translator = str.maketrans('', '', string.punctuation)
data = data.translate(translator)
print(data)

```

## 8、为给定句子生成 N-gram
文本的`N-gram`是一种基于文本序列的常用分析方法。`N-gram`是由连续的`N`个项（可以是字符、词语等）组成的序列。在自然语言处理中，常用的是基于词语的`N-gram`。

对于一个文本序列，`N-gram`将文本切分为连续的N个词语组成的序列。`N`表示`N-gram`的大小，通常是一个正整数。例如，对于句子 `"I love natural language processing"`，当`N=2`时，它的`2-gram`序列是 `["I love", "love natural", "natural language", "language processing"]`。

`N-gram`可用于许多自然语言处理任务，如语言建模、信息检索、机器翻译和文本生成。通过分析`N-gram`序列的频率和出现模式，我们可以推断出词语之间的关系、常见短语的组合和上下文信息。

在`Python`中，可以使用`NLTK（自然语言工具包）`或者 `TextBlob` 库来计算`N-gram`。下面是一个使用`NLTK`计算文本`N-gram`的示例代码：

```python

import nltk
from nltk.util import ngrams
# Function to generate n-grams from sentences.
def extract_ngrams(data, num):
    n_grams = ngrams(nltk.word_tokenize(data), num)
    return [ ' '.join(grams) for grams in n_grams]
data = 'A class is a blueprint for the object.'
print("1-gram: ", extract_ngrams(data, 1))
print("2-gram: ", extract_ngrams(data, 2))
print("3-gram: ", extract_ngrams(data, 3))
print("4-gram: ", extract_ngrams(data, 4))

```
输入结果如下：
```python
1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object']
2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object']
3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object']
4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object']
```

## 参考资料
https://developer.aliyun.com/article/931887
